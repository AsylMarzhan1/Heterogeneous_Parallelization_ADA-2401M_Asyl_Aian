{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Задание 1 (25 баллов)**\n",
        "Реализуйте CUDA-программу для вычисления суммы элементов массива с\n",
        "использованием глобальной памяти. Сравните результат и время выполнения с\n",
        "последовательной реализацией на CPU для массива размером 100 000 элементов."
      ],
      "metadata": {
        "id": "M9jVrYxFhtFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZcHuPAmhow-",
        "outputId": "722ce0b7-ab82-4f4b-b614-b2e2e951b96e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile task1.cu\n",
        "#include <cuda_runtime.h>                           // подключаем CUDA Runtime API\n",
        "#include <cstdio>                                   // подключаем printf\n",
        "#include <vector>                                   // подключаем std::vector\n",
        "#include <random>                                   // подключаем генератор случайных чисел\n",
        "#include <chrono>                                   // подключаем средства измерения времени\n",
        "#include <cstdint>                                  // подключаем типы фиксированной разрядности\n",
        "\n",
        "#define CHECK_CUDA(call) do {                       /* макрос для проверки CUDA-ошибок */ \\\n",
        "  cudaError_t err = (call);                         /* выполняем вызов CUDA и сохраняем код ошибки */ \\\n",
        "  if (err != cudaSuccess) {                         /* если ошибка не равна cudaSuccess */ \\\n",
        "    printf(\"CUDA error %s:%d: %s\\n\",                /* печатаем сообщение об ошибке */ \\\n",
        "           __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "    return 1;                                       /* завершаем программу с ошибкой */ \\\n",
        "  }                                                 /* конец if */ \\\n",
        "} while(0)                                          /* конец макроса */\n",
        "\n",
        "// --- CUDA kernel: сумма через global memory (atomicAdd) ---\n",
        "__global__ void sum_global_atomic(const int* a, int n, unsigned long long* out_sum) { // ядро: массив a, размер n, сумма out_sum\n",
        "  int tid = blockIdx.x * blockDim.x + threadIdx.x;                                   // вычисляем глобальный индекс потока\n",
        "  int stride = blockDim.x * gridDim.x;                                               // вычисляем шаг сетки (общее число потоков)\n",
        "  for (int i = tid; i < n; i += stride) {                                            // grid-stride loop: каждый поток суммирует несколько элементов\n",
        "    atomicAdd(out_sum, (unsigned long long)a[i]);                                    // атомарно добавляем элемент к сумме (global memory)\n",
        "  }                                                                                   // конец цикла\n",
        "}                                                                                     // конец ядра\n",
        "\n",
        "int main() {                                                                          // начало main\n",
        "  const int N = 100000;                                                               // размер массива (100 000)\n",
        "  std::vector<int> h_a(N);                                                            // создаем host-массив на CPU\n",
        "  std::mt19937 rng(123);                                                              // фиксируем seed для воспроизводимости\n",
        "  std::uniform_int_distribution<int> dist(0, 9);                                      // распределение значений 0..9\n",
        "  for (int i = 0; i < N; ++i) h_a[i] = dist(rng);                                     // заполняем массив случайными числами\n",
        "\n",
        "  auto cpu_t0 = std::chrono::high_resolution_clock::now();                            // старт таймера CPU\n",
        "  long long cpu_sum = 0;                                                              // сумма на CPU (signed long long)\n",
        "  for (int i = 0; i < N; ++i) cpu_sum += (long long)h_a[i];                           // последовательное суммирование на CPU\n",
        "  auto cpu_t1 = std::chrono::high_resolution_clock::now();                            // стоп таймера CPU\n",
        "  double cpu_ms = std::chrono::duration<double, std::milli>(cpu_t1 - cpu_t0).count(); // время CPU в миллисекундах\n",
        "\n",
        "  int* d_a = nullptr;                                                                 // указатель на массив на GPU\n",
        "  unsigned long long* d_sum = nullptr;                                                // указатель на сумму на GPU (unsigned long long)\n",
        "  CHECK_CUDA(cudaMalloc((void**)&d_a, N * sizeof(int)));                              // выделяем память на GPU под массив\n",
        "  CHECK_CUDA(cudaMalloc((void**)&d_sum, sizeof(unsigned long long)));                 // выделяем память на GPU под сумму\n",
        "  CHECK_CUDA(cudaMemcpy(d_a, h_a.data(), N * sizeof(int), cudaMemcpyHostToDevice));   // копируем массив CPU->GPU\n",
        "  CHECK_CUDA(cudaMemset(d_sum, 0, sizeof(unsigned long long)));                       // обнуляем сумму на GPU\n",
        "\n",
        "  int threads = 256;                                                                  // число потоков в блоке\n",
        "  int blocks = (N + threads - 1) / threads;                                           // считаем блоки, чтобы покрыть N элементов\n",
        "  if (blocks > 1024) blocks = 1024;                                                   // ограничиваем число блоков разумным пределом\n",
        "\n",
        "  cudaEvent_t start, stop;                                                            // объявляем CUDA-события для тайминга\n",
        "  CHECK_CUDA(cudaEventCreate(&start));                                                // создаем событие start\n",
        "  CHECK_CUDA(cudaEventCreate(&stop));                                                 // создаем событие stop\n",
        "  CHECK_CUDA(cudaEventRecord(start));                                                 // записываем старт таймера (в GPU очередь)\n",
        "\n",
        "  sum_global_atomic<<<blocks, threads>>>(d_a, N, d_sum);                              // запускаем ядро суммирования\n",
        "\n",
        "  CHECK_CUDA(cudaEventRecord(stop));                                                  // записываем стоп таймера\n",
        "  CHECK_CUDA(cudaEventSynchronize(stop));                                             // ждем завершения ядра\n",
        "  CHECK_CUDA(cudaGetLastError());                                                     // проверяем ошибку запуска ядра\n",
        "\n",
        "  float gpu_ms = 0.0f;                                                                // переменная для времени GPU\n",
        "  CHECK_CUDA(cudaEventElapsedTime(&gpu_ms, start, stop));                             // получаем время выполнения ядра (мс)\n",
        "\n",
        "  unsigned long long gpu_sum_u = 0;                                                   // переменная для суммы, пришедшей с GPU\n",
        "  CHECK_CUDA(cudaMemcpy(&gpu_sum_u, d_sum, sizeof(unsigned long long), cudaMemcpyDeviceToHost)); // копируем сумму GPU->CPU\n",
        "  long long gpu_sum = (long long)gpu_sum_u;                                           // приводим к signed для сравнения (у нас сумма неотрицательная)\n",
        "\n",
        "  printf(\"CPU sum      = %lld\\n\", cpu_sum);                                           // печатаем сумму CPU\n",
        "  printf(\"GPU sum      = %lld\\n\", gpu_sum);                                           // печатаем сумму GPU\n",
        "  printf(\"CPU time     = %.3f ms\\n\", cpu_ms);                                         // печатаем время CPU\n",
        "  printf(\"GPU kernel   = %.3f ms (global memory + atomicAdd)\\n\", gpu_ms);             // печатаем время ядра GPU\n",
        "\n",
        "  CHECK_CUDA(cudaEventDestroy(start));                                                // удаляем событие start\n",
        "  CHECK_CUDA(cudaEventDestroy(stop));                                                 // удаляем событие stop\n",
        "  CHECK_CUDA(cudaFree(d_a));                                                          // освобождаем память массива на GPU\n",
        "  CHECK_CUDA(cudaFree(d_sum));                                                        // освобождаем память суммы на GPU\n",
        "  CHECK_CUDA(cudaDeviceReset());                                                      // сбрасываем устройство (полезно в Colab)\n",
        "\n",
        "  return 0;                                                                           // завершаем программу успешно\n",
        "}                                                                                     // конец main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -std=c++17 task1.cu -o task1 \\\n",
        "  -gencode arch=compute_75,code=sm_75\n",
        "!./task1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdsJwnacjEXK",
        "outputId": "291478c7-5ec7-4bf9-f276-5740cfae6f3b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU sum      = 451583\n",
            "GPU sum      = 451583\n",
            "CPU time     = 0.040 ms\n",
            "GPU kernel   = 0.264 ms (global memory + atomicAdd)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Задание 2 (25 баллов)**\n",
        "Реализуйте CUDA-программу для вычисления префиксной суммы (сканирования)\n",
        "массива с использованием разделяемой памяти. Сравните время выполнения с\n",
        "последовательной реализацией на CPU для массива размером 1 000 000 элементов."
      ],
      "metadata": {
        "id": "u2WmGlYFhtX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task2.cu\n",
        "#include <cuda_runtime.h>                           // подключаем CUDA Runtime API\n",
        "#include <cstdio>                                   // подключаем printf\n",
        "#include <vector>                                   // подключаем std::vector\n",
        "#include <random>                                   // подключаем генератор случайных чисел\n",
        "#include <chrono>                                   // подключаем измерение времени на CPU\n",
        "#include <algorithm>                                // подключаем std::min\n",
        "\n",
        "#define CHECK_CUDA(call) do {                       /* макрос для проверки ошибок CUDA */ \\\n",
        "  cudaError_t err = (call);                         /* выполняем CUDA-вызов */ \\\n",
        "  if (err != cudaSuccess) {                         /* если вернулась ошибка */ \\\n",
        "    printf(\"CUDA error %s:%d: %s\\n\",                /* печатаем где и какая ошибка */ \\\n",
        "           __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "    return 1;                                       /* выходим с кодом ошибки */ \\\n",
        "  }                                                 /* конец if */ \\\n",
        "} while(0)                                          /* конец макроса */\n",
        "\n",
        "// --- Kernel 1: блочный inclusive scan (префиксная сумма) в shared memory + запись суммы блока ---\n",
        "__global__ void block_inclusive_scan(const int* in, int* out, int n, int* block_sums) { // ядро: вход, выход, размер, суммы блоков\n",
        "  extern __shared__ int sh[];                                                         // выделяем динамическую shared memory под блок\n",
        "  int tid = threadIdx.x;                                                              // индекс потока внутри блока\n",
        "  int gid = blockIdx.x * blockDim.x + tid;                                            // глобальный индекс элемента массива\n",
        "\n",
        "  int x = 0;                                                                          // переменная для значения элемента (или 0)\n",
        "  if (gid < n) x = in[gid];                                                           // если индекс в пределах массива — берем значение\n",
        "  sh[tid] = x;                                                                        // кладем значение в shared memory\n",
        "  __syncthreads();                                                                    // синхронизируем потоки блока (все должны загрузить sh)\n",
        "\n",
        "  for (int offset = 1; offset < blockDim.x; offset <<= 1) {                           // цикл по степеням двойки: 1,2,4,8...\n",
        "    int add = 0;                                                                      // переменная для прибавки\n",
        "    if (tid >= offset) add = sh[tid - offset];                                        // берем значение слева на offset, если оно существует\n",
        "    __syncthreads();                                                                  // синхронизация, чтобы все прочитали старые sh\n",
        "    sh[tid] += add;                                                                   // выполняем шаг Hillis–Steele (inclusive scan)\n",
        "    __syncthreads();                                                                  // синхронизация, чтобы обновления завершились\n",
        "  }                                                                                   // конец цикла scan\n",
        "\n",
        "  if (gid < n) out[gid] = sh[tid];                                                    // записываем результат scan обратно в global memory\n",
        "\n",
        "  int block_start = blockIdx.x * blockDim.x;                                          // начало диапазона элементов, которые обрабатывает блок\n",
        "  int valid = min(blockDim.x, n - block_start);                                       // сколько элементов реально есть в блоке (учет хвоста)\n",
        "  if (tid == valid - 1) block_sums[blockIdx.x] = sh[tid];                             // последний валидный поток записывает сумму блока\n",
        "}                                                                                     // конец kernel 1\n",
        "\n",
        "// --- Kernel 2: inclusive scan массива block_sums (ожидаем, что он помещается в один блок) ---\n",
        "__global__ void scan_block_sums(int* data, int m) {                                   // ядро: делаем inclusive scan для m элементов\n",
        "  extern __shared__ int sh[];                                                         // shared memory под m значений\n",
        "  int tid = threadIdx.x;                                                              // индекс потока в блоке\n",
        "\n",
        "  int x = 0;                                                                          // значение по умолчанию (для потоков вне диапазона)\n",
        "  if (tid < m) x = data[tid];                                                         // если tid < m — берем соответствующий block sum\n",
        "  sh[tid] = x;                                                                        // кладем в shared\n",
        "  __syncthreads();                                                                    // синхронизация\n",
        "\n",
        "  for (int offset = 1; offset < blockDim.x; offset <<= 1) {                           // такой же Hillis–Steele scan\n",
        "    int add = 0;                                                                      // прибавка\n",
        "    if (tid >= offset) add = sh[tid - offset];                                        // берем левый элемент\n",
        "    __syncthreads();                                                                  // синхронизация перед записью\n",
        "    sh[tid] += add;                                                                   // обновляем scan\n",
        "    __syncthreads();                                                                  // синхронизация после записи\n",
        "  }                                                                                   // конец цикла\n",
        "\n",
        "  if (tid < m) data[tid] = sh[tid];                                                   // записываем обратно только валидные элементы\n",
        "}                                                                                     // конец kernel 2\n",
        "\n",
        "// --- Kernel 3: добавление оффсетов блоков к результату scan каждого блока ---\n",
        "__global__ void add_block_offsets(int* out, int n, const int* block_prefix, int m) {  // ядро: out + префиксы блоков\n",
        "  int tid = threadIdx.x;                                                              // индекс потока в блоке\n",
        "  int gid = blockIdx.x * blockDim.x + tid;                                            // глобальный индекс\n",
        "\n",
        "  if (gid >= n) return;                                                               // если вышли за пределы — выходим\n",
        "\n",
        "  int b = blockIdx.x;                                                                 // номер блока текущего элемента\n",
        "  int offset = 0;                                                                     // оффсет по умолчанию для первого блока\n",
        "  if (b > 0 && b - 1 < m) offset = block_prefix[b - 1];                               // для блока b берем сумму всех предыдущих блоков\n",
        "\n",
        "  out[gid] += offset;                                                                 // добавляем оффсет (получаем глобальную префиксную сумму)\n",
        "}                                                                                     // конец kernel 3\n",
        "\n",
        "int main() {                                                                          // начало main\n",
        "  const int N = 1000000;                                                              // размер массива по заданию (1 000 000)\n",
        "  std::vector<int> h_in(N);                                                           // входной массив на CPU\n",
        "  std::mt19937 rng(123);                                                              // seed для воспроизводимости\n",
        "  std::uniform_int_distribution<int> dist(0, 9);                                      // значения 0..9 (без переполнения int на префиксе)\n",
        "  for (int i = 0; i < N; ++i) h_in[i] = dist(rng);                                    // заполняем вход\n",
        "\n",
        "  // --- CPU последовательный inclusive prefix sum и его время ---\n",
        "  std::vector<int> h_cpu(N);                                                          // массив результата на CPU\n",
        "  auto cpu_t0 = std::chrono::high_resolution_clock::now();                            // старт таймера CPU\n",
        "  int running = 0;                                                                    // текущая накопленная сумма\n",
        "  for (int i = 0; i < N; ++i) {                                                       // идем по всем элементам\n",
        "    running += h_in[i];                                                               // добавляем текущий элемент\n",
        "    h_cpu[i] = running;                                                               // сохраняем inclusive prefix sum\n",
        "  }                                                                                   // конец цикла CPU\n",
        "  auto cpu_t1 = std::chrono::high_resolution_clock::now();                            // стоп таймера CPU\n",
        "  double cpu_ms = std::chrono::duration<double, std::milli>(cpu_t1 - cpu_t0).count(); // время CPU в мс\n",
        "\n",
        "  // --- Память на GPU ---\n",
        "  int* d_in = nullptr;                                                                // указатель на вход на GPU\n",
        "  int* d_out = nullptr;                                                               // указатель на выход на GPU\n",
        "  CHECK_CUDA(cudaMalloc((void**)&d_in, N * sizeof(int)));                             // выделяем память под вход\n",
        "  CHECK_CUDA(cudaMalloc((void**)&d_out, N * sizeof(int)));                            // выделяем память под выход\n",
        "  CHECK_CUDA(cudaMemcpy(d_in, h_in.data(), N * sizeof(int), cudaMemcpyHostToDevice)); // копируем вход CPU->GPU\n",
        "\n",
        "  // --- Параметры запуска ---\n",
        "  int threads = 1024;                                                                 // число потоков в блоке (максимум для многих GPU)\n",
        "  int blocks = (N + threads - 1) / threads;                                           // число блоков для покрытия N\n",
        "  int m = blocks;                                                                     // размер массива сумм блоков\n",
        "\n",
        "  int* d_block_sums = nullptr;                                                        // указатель на суммы блоков\n",
        "  CHECK_CUDA(cudaMalloc((void**)&d_block_sums, m * sizeof(int)));                     // выделяем память под block_sums\n",
        "\n",
        "  // --- CUDA events для замера времени всех GPU-ядeр (kernel time total) ---\n",
        "  cudaEvent_t start, stop;                                                            // события CUDA\n",
        "  CHECK_CUDA(cudaEventCreate(&start));                                                // создаем start\n",
        "  CHECK_CUDA(cudaEventCreate(&stop));                                                 // создаем stop\n",
        "  CHECK_CUDA(cudaEventRecord(start));                                                 // стартуем измерение\n",
        "\n",
        "  // --- Kernel 1: блочный scan + суммы блоков ---\n",
        "  block_inclusive_scan<<<blocks, threads, threads * (int)sizeof(int)>>>(              // запускаем kernel 1 с dynamic shared memory\n",
        "    d_in, d_out, N, d_block_sums);                                                    // передаем аргументы\n",
        "  CHECK_CUDA(cudaGetLastError());                                                     // проверяем ошибку запуска\n",
        "\n",
        "  // --- Kernel 2: scan массива block_sums в одном блоке (m обычно < 1024 здесь) ---\n",
        "  int t2 = 1;                                                                         // переменная для потоков kernel 2\n",
        "  while (t2 < m) t2 <<= 1;                                                            // округляем число потоков вверх до степени двойки\n",
        "  if (t2 > 1024) t2 = 1024;                                                           // на всякий случай ограничиваем 1024\n",
        "  scan_block_sums<<<1, t2, t2 * (int)sizeof(int)>>>(d_block_sums, m);                 // сканируем суммы блоков\n",
        "  CHECK_CUDA(cudaGetLastError());                                                     // проверяем запуск\n",
        "\n",
        "  // --- Kernel 3: добавляем оффсеты блоков ко всем элементам (кроме первого блока оффсет > 0) ---\n",
        "  add_block_offsets<<<blocks, threads>>>(d_out, N, d_block_sums, m);                  // добавляем префиксы блоков\n",
        "  CHECK_CUDA(cudaGetLastError());                                                     // проверяем запуск\n",
        "\n",
        "  CHECK_CUDA(cudaEventRecord(stop));                                                  // записываем stop\n",
        "  CHECK_CUDA(cudaEventSynchronize(stop));                                             // ждем окончания всех GPU работ\n",
        "\n",
        "  float gpu_ms = 0.0f;                                                                // время GPU в мс\n",
        "  CHECK_CUDA(cudaEventElapsedTime(&gpu_ms, start, stop));                             // получаем elapsed time (все ядра вместе)\n",
        "\n",
        "  // --- Скачиваем результат с GPU и проверяем корректность ---\n",
        "  std::vector<int> h_gpu(N);                                                          // буфер результата на CPU\n",
        "  CHECK_CUDA(cudaMemcpy(h_gpu.data(), d_out, N * sizeof(int), cudaMemcpyDeviceToHost)); // копируем GPU->CPU\n",
        "\n",
        "  int ok = 1;                                                                         // флаг корректности\n",
        "  for (int i = 0; i < N; ++i) {                                                       // проверяем все элементы\n",
        "    if (h_gpu[i] != h_cpu[i]) {                                                       // если нашли несовпадение\n",
        "      ok = 0;                                                                         // отмечаем ошибку\n",
        "      printf(\"Mismatch at i=%d: CPU=%d GPU=%d\\n\", i, h_cpu[i], h_gpu[i]);             // выводим где ошибка\n",
        "      break;                                                                          // выходим из цикла\n",
        "    }                                                                                 // конец if\n",
        "  }                                                                                   // конец проверки\n",
        "\n",
        "  // --- Вывод времени и статуса ---\n",
        "  printf(\"CPU time     = %.3f ms (sequential scan)\\n\", cpu_ms);                       // выводим время CPU\n",
        "  printf(\"GPU kernels  = %.3f ms (shared-memory block scan + block offsets)\\n\", gpu_ms); // выводим время всех GPU-ядeр\n",
        "\n",
        "  // --- Освобождение ресурсов ---\n",
        "  CHECK_CUDA(cudaEventDestroy(start));                                                // удаляем событие start\n",
        "  CHECK_CUDA(cudaEventDestroy(stop));                                                 // удаляем событие stop\n",
        "  CHECK_CUDA(cudaFree(d_in));                                                         // освобождаем вход на GPU\n",
        "  CHECK_CUDA(cudaFree(d_out));                                                        // освобождаем выход на GPU\n",
        "  CHECK_CUDA(cudaFree(d_block_sums));                                                 // освобождаем block sums\n",
        "  CHECK_CUDA(cudaDeviceReset());                                                      // сброс устройства (полезно в Colab)\n",
        "\n",
        "  return 0;                                                                           // успешный выход\n",
        "}                                                                                     // конец main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy2pQCB-htpV",
        "outputId": "79106bf2-5158-4cca-f0ff-9c8a844fa82d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -std=c++17 task2.cu -o task2 \\\n",
        "  -gencode arch=compute_75,code=sm_75\n",
        "!./task2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLIkZzgCncOm",
        "outputId": "61ec5c4f-681d-4cc5-be12-c35335552320"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mtask2.cu(143)\u001b[0m: \u001b[01;35mwarning\u001b[0m #550-D: variable \u001b[01m\"ok\"\u001b[0m was set but never used\n",
            "    int ok = 1;\n",
            "        ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "CPU time     = 0.834 ms (sequential scan)\n",
            "GPU kernels  = 0.413 ms (shared-memory block scan + block offsets)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Задание 3 (25 баллов)**\n",
        "Реализуйте гибридную программу, в которой обработка массива выполняется\n",
        "параллельно на CPU и GPU. Первую часть массива обработайте на CPU, вторую — на\n",
        "GPU. Сравните время выполнения CPU-, GPU- и гибридной реализаций."
      ],
      "metadata": {
        "id": "ZUGMh2Hnht2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task3.cu\n",
        "#include <cuda_runtime.h>                           // CUDA runtime\n",
        "#include <cstdio>                                   // printf\n",
        "#include <vector>                                   // vector\n",
        "#include <random>                                   // rng\n",
        "#include <chrono>                                   // chrono\n",
        "\n",
        "#define CHECK_CUDA(call) do {                       /* проверка ошибок CUDA */ \\\n",
        "  cudaError_t err = (call);                         /* вызов */ \\\n",
        "  if (err != cudaSuccess) {                         /* ошибка? */ \\\n",
        "    printf(\"CUDA error %s:%d: %s\\n\",                /* печать */ \\\n",
        "           __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "    return 1;                                       /* выход */ \\\n",
        "  }                                                 \\\n",
        "} while(0)\n",
        "\n",
        "// GPU kernel: сумма части массива (atomicAdd)\n",
        "__global__ void sum_part_atomic(const int* a, int n, unsigned long long* out_sum) { // ядро\n",
        "  int tid = blockIdx.x * blockDim.x + threadIdx.x;                                 // id потока\n",
        "  int stride = blockDim.x * gridDim.x;                                             // шаг\n",
        "  for (int i = tid; i < n; i += stride) {                                          // цикл по элементам\n",
        "    atomicAdd(out_sum, (unsigned long long)a[i]);                                  // атомарное сложение\n",
        "  }                                                                                // конец цикла\n",
        "}                                                                                  // конец ядра\n",
        "\n",
        "int main() {                                                                       // main\n",
        "  const int N = 1'000'000;                                                         // размер массива\n",
        "  const int HALF = N / 2;                                                          // половина\n",
        "\n",
        "  std::vector<int> h_a(N);                                                         // массив на CPU\n",
        "  std::mt19937 rng(123);                                                           // seed\n",
        "  std::uniform_int_distribution<int> dist(0, 9);                                   // 0..9\n",
        "  for (int i = 0; i < N; ++i) h_a[i] = dist(rng);                                  // заполнение\n",
        "\n",
        "  // ---------------- CPU-only ----------------\n",
        "  auto cpu0 = std::chrono::high_resolution_clock::now();                           // старт CPU-only\n",
        "  long long cpu_sum = 0;                                                           // сумма CPU\n",
        "  for (int i = 0; i < N; ++i) cpu_sum += (long long)h_a[i];                        // суммирование\n",
        "  auto cpu1 = std::chrono::high_resolution_clock::now();                           // стоп CPU-only\n",
        "  double cpu_ms = std::chrono::duration<double, std::milli>(cpu1 - cpu0).count();  // время CPU-only\n",
        "\n",
        "  // ---------------- GPU memory ----------------\n",
        "  int* d_a = nullptr;                                                              // массив на GPU\n",
        "  unsigned long long* d_sum = nullptr;                                             // сумма на GPU\n",
        "  CHECK_CUDA(cudaMalloc(&d_a, N * sizeof(int)));                                   // malloc массива\n",
        "  CHECK_CUDA(cudaMalloc(&d_sum, sizeof(unsigned long long)));                      // malloc суммы\n",
        "  CHECK_CUDA(cudaMemcpy(d_a, h_a.data(), N * sizeof(int), cudaMemcpyHostToDevice)); // копирование H2D\n",
        "\n",
        "  // ---------------- GPU-only (kernel time) ----------------\n",
        "  CHECK_CUDA(cudaMemset(d_sum, 0, sizeof(unsigned long long)));                    // обнуление\n",
        "  cudaEvent_t g0, g1;                                                              // события\n",
        "  CHECK_CUDA(cudaEventCreate(&g0));                                                // create\n",
        "  CHECK_CUDA(cudaEventCreate(&g1));                                                // create\n",
        "  CHECK_CUDA(cudaEventRecord(g0));                                                 // старт\n",
        "  sum_part_atomic<<<256, 256>>>(d_a, N, d_sum);                                    // kernel\n",
        "  CHECK_CUDA(cudaEventRecord(g1));                                                 // стоп\n",
        "  CHECK_CUDA(cudaEventSynchronize(g1));                                            // ждать\n",
        "  float gpu_ms = 0.0f;                                                             // время\n",
        "  CHECK_CUDA(cudaEventElapsedTime(&gpu_ms, g0, g1));                               // elapsed\n",
        "  unsigned long long gpu_sum_u = 0;                                                // сумма GPU\n",
        "  CHECK_CUDA(cudaMemcpy(&gpu_sum_u, d_sum, sizeof(unsigned long long), cudaMemcpyDeviceToHost)); // D2H\n",
        "  long long gpu_sum = (long long)gpu_sum_u;                                        // cast\n",
        "\n",
        "  // ---------------- Hybrid PARALLEL with separate timings ----------------\n",
        "  CHECK_CUDA(cudaMemset(d_sum, 0, sizeof(unsigned long long)));                    // обнуление для hybrid\n",
        "\n",
        "  cudaEvent_t hg0, hg1;                                                            // события для GPU части в hybrid\n",
        "  CHECK_CUDA(cudaEventCreate(&hg0));                                               // create\n",
        "  CHECK_CUDA(cudaEventCreate(&hg1));                                               // create\n",
        "\n",
        "  auto hyb0 = std::chrono::high_resolution_clock::now();                           // старт общего hybrid (wall time)\n",
        "\n",
        "  CHECK_CUDA(cudaEventRecord(hg0));                                                // старт kernel таймера hybrid-GPU\n",
        "  sum_part_atomic<<<256, 256>>>(d_a + HALF, HALF, d_sum);                          // GPU обрабатывает 2-ю половину (асинхронно)\n",
        "  CHECK_CUDA(cudaEventRecord(hg1));                                                // стоп kernel таймера hybrid-GPU (в той же очереди)\n",
        "\n",
        "  auto hcpu0 = std::chrono::high_resolution_clock::now();                          // старт таймера hybrid-CPU\n",
        "  long long cpu_part = 0;                                                          // сумма CPU части\n",
        "  for (int i = 0; i < HALF; ++i) cpu_part += (long long)h_a[i];                    // CPU считает 1-ю половину параллельно GPU\n",
        "  auto hcpu1 = std::chrono::high_resolution_clock::now();                          // стоп таймера hybrid-CPU\n",
        "  double hybrid_cpu_ms = std::chrono::duration<double, std::milli>(hcpu1 - hcpu0).count(); // время CPU-части\n",
        "\n",
        "  CHECK_CUDA(cudaEventSynchronize(hg1));                                           // ждём завершения GPU kernel в hybrid\n",
        "  float hybrid_gpu_kernel_ms = 0.0f;                                               // время kernel в hybrid\n",
        "  CHECK_CUDA(cudaEventElapsedTime(&hybrid_gpu_kernel_ms, hg0, hg1));               // elapsed kernel hybrid-GPU\n",
        "\n",
        "  unsigned long long gpu_part_u = 0;                                               // сумма GPU части\n",
        "  CHECK_CUDA(cudaMemcpy(&gpu_part_u, d_sum, sizeof(unsigned long long), cudaMemcpyDeviceToHost)); // D2H\n",
        "  long long gpu_part = (long long)gpu_part_u;                                      // cast\n",
        "\n",
        "  long long hybrid_sum = cpu_part + gpu_part;                                      // итоговая сумма hybrid\n",
        "\n",
        "  auto hyb1 = std::chrono::high_resolution_clock::now();                           // стоп общего hybrid\n",
        "  double hybrid_total_ms = std::chrono::duration<double, std::milli>(hyb1 - hyb0).count(); // общее время hybrid\n",
        "\n",
        "  // ---------------- Print ----------------\n",
        "  printf(\"CPU only sum        = %lld, time = %.3f ms\\n\", cpu_sum, cpu_ms);          // CPU-only\n",
        "  printf(\"GPU only sum        = %lld, time = %.3f ms (kernel)\\n\", gpu_sum, gpu_ms); // GPU-only kernel\n",
        "  printf(\"Hybrid CPU part     = %lld, time = %.3f ms\\n\", cpu_part, hybrid_cpu_ms);  // hybrid CPU часть + время\n",
        "  printf(\"Hybrid GPU part     = %lld, time = %.3f ms (kernel)\\n\", gpu_part, hybrid_gpu_kernel_ms); // hybrid GPU часть + kernel time\n",
        "  printf(\"Hybrid total sum    = %lld, time = %.3f ms (wall)\\n\", hybrid_sum, hybrid_total_ms); // hybrid total wall time\n",
        "\n",
        "  // ---------------- Cleanup ----------------\n",
        "  CHECK_CUDA(cudaEventDestroy(g0));                                                // destroy\n",
        "  CHECK_CUDA(cudaEventDestroy(g1));                                                // destroy\n",
        "  CHECK_CUDA(cudaEventDestroy(hg0));                                               // destroy\n",
        "  CHECK_CUDA(cudaEventDestroy(hg1));                                               // destroy\n",
        "  CHECK_CUDA(cudaFree(d_a));                                                       // free\n",
        "  CHECK_CUDA(cudaFree(d_sum));                                                     // free\n",
        "  CHECK_CUDA(cudaDeviceReset());                                                   // reset\n",
        "\n",
        "  return 0;                                                                        // exit\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVURftuShuGD",
        "outputId": "6518eab9-e4a9-44ff-ffa7-c599548a44c0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -std=c++17 task3.cu -o task3 \\\n",
        "  -gencode arch=compute_75,code=sm_75\n",
        "!./task3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi1cuZVdpIMi",
        "outputId": "32a1c74a-4898-4ee9-beb9-3de88bda4f99"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU only sum        = 4505398, time = 0.293 ms\n",
            "GPU only sum        = 4505398, time = 1.509 ms (kernel)\n",
            "Hybrid CPU part     = 2254182, time = 0.354 ms\n",
            "Hybrid GPU part     = 2251216, time = 0.741 ms (kernel)\n",
            "Hybrid total sum    = 4505398, time = 0.773 ms (wall)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Задание 4 (25 баллов)**\n",
        "Реализуйте распределённую программу с использованием MPI для обработки массива\n",
        "данных. Разделите массив между процессами, выполните вычисления локально и\n",
        "соберите результаты. Проведите замеры времени выполнения для 2, 4 и 8 процессов."
      ],
      "metadata": {
        "id": "L-5cBzC_hucp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq update\n",
        "!apt-get -qq install -y mpich"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-y0OhdAhu0V",
        "outputId": "f892dfb2-4bc1-4cc6-81d7-e6184bfed247"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libslurm37.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../libslurm37_21.08.5-2ubuntu1_amd64.deb ...\n",
            "Unpacking libslurm37 (21.08.5-2ubuntu1) ...\n",
            "Selecting previously unselected package hwloc-nox.\n",
            "Preparing to unpack .../hwloc-nox_2.7.0-2ubuntu1_amd64.deb ...\n",
            "Unpacking hwloc-nox (2.7.0-2ubuntu1) ...\n",
            "Selecting previously unselected package libmpich12:amd64.\n",
            "Preparing to unpack .../libmpich12_4.0-3_amd64.deb ...\n",
            "Unpacking libmpich12:amd64 (4.0-3) ...\n",
            "Selecting previously unselected package mpich.\n",
            "Preparing to unpack .../archives/mpich_4.0-3_amd64.deb ...\n",
            "Unpacking mpich (4.0-3) ...\n",
            "Selecting previously unselected package libmpich-dev:amd64.\n",
            "Preparing to unpack .../libmpich-dev_4.0-3_amd64.deb ...\n",
            "Unpacking libmpich-dev:amd64 (4.0-3) ...\n",
            "Setting up libslurm37 (21.08.5-2ubuntu1) ...\n",
            "Setting up hwloc-nox (2.7.0-2ubuntu1) ...\n",
            "Setting up libmpich12:amd64 (4.0-3) ...\n",
            "Setting up mpich (4.0-3) ...\n",
            "Setting up libmpich-dev:amd64 (4.0-3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task4.cpp\n",
        "#include <mpi.h>                                      // подключаем библиотеку MPI\n",
        "#include <cstdio>                                     // подключаем printf\n",
        "#include <vector>                                     // подключаем std::vector\n",
        "#include <random>                                     // подключаем генератор случайных чисел\n",
        "#include <cstdint>                                    // подключаем целочисленные типы\n",
        "#include <algorithm>                                  // подключаем std::min\n",
        "\n",
        "int main(int argc, char** argv) {                     // главная функция программы (argc/argv нужны MPI)\n",
        "  MPI_Init(&argc, &argv);                             // инициализируем MPI-среду\n",
        "\n",
        "  int rank = 0;                                       // переменная для номера процесса\n",
        "  int size = 0;                                       // переменная для количества процессов\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);               // получаем номер текущего процесса (rank)\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &size);               // получаем общее число процессов (size)\n",
        "\n",
        "  const int N = 10'000'000;                           // общий размер массива (можно менять для экспериментов)\n",
        "  std::vector<int> sendcounts(size);                  // массив: сколько элементов отправить каждому процессу\n",
        "  std::vector<int> displs(size);                      // массив: смещения (offset) для Scatterv\n",
        "\n",
        "  int base = N / size;                                // базовый размер куска на процесс\n",
        "  int rem  = N % size;                                // остаток, который распределим по первым процессам\n",
        "\n",
        "  for (int p = 0; p < size; ++p) {                    // цикл по всем процессам\n",
        "    sendcounts[p] = base + (p < rem ? 1 : 0);         // первым rem процессам даём на 1 элемент больше\n",
        "  }                                                   // конец цикла\n",
        "\n",
        "  displs[0] = 0;                                      // смещение для первого процесса = 0\n",
        "  for (int p = 1; p < size; ++p) {                    // цикл для вычисления смещений\n",
        "    displs[p] = displs[p - 1] + sendcounts[p - 1];    // смещение = сумма всех предыдущих sendcounts\n",
        "  }                                                   // конец цикла\n",
        "\n",
        "  std::vector<int> global;                            // глобальный массив (будет только на rank 0)\n",
        "  if (rank == 0) {                                    // если это главный процесс\n",
        "    global.resize(N);                                 // выделяем память под весь массив\n",
        "    std::mt19937 rng(123);                            // фиксируем seed для воспроизводимости\n",
        "    std::uniform_int_distribution<int> dist(0, 9);    // распределение значений 0..9\n",
        "    for (int i = 0; i < N; ++i) global[i] = dist(rng);// заполняем массив случайными числами\n",
        "  }                                                   // конец if (rank==0)\n",
        "\n",
        "  int local_n = sendcounts[rank];                     // сколько элементов получит текущий процесс\n",
        "  std::vector<int> local(local_n);                    // локальный массив для текущего процесса\n",
        "\n",
        "  MPI_Barrier(MPI_COMM_WORLD);                        // барьер: все процессы стартуют измерение синхронно\n",
        "  double t0 = MPI_Wtime();                            // старт таймера MPI (секунды)\n",
        "\n",
        "  MPI_Scatterv(                                       // распределяем части массива по процессам\n",
        "    rank == 0 ? global.data() : nullptr,              // буфер отправки (только у rank 0, иначе nullptr)\n",
        "    sendcounts.data(),                                // сколько отправлять каждому процессу\n",
        "    displs.data(),                                    // смещения в глобальном массиве\n",
        "    MPI_INT,                                          // тип элементов отправки (int)\n",
        "    local.data(),                                     // буфер приёма локальных данных\n",
        "    local_n,                                          // сколько принять (для текущего процесса)\n",
        "    MPI_INT,                                          // тип элементов приёма\n",
        "    0,                                                // корневой процесс (root) = 0\n",
        "    MPI_COMM_WORLD                                    // коммуникатор всех процессов\n",
        "  );                                                  // конец Scatterv\n",
        "\n",
        "  long long local_sum = 0;                            // локальная сумма на процессе\n",
        "  for (int i = 0; i < local_n; ++i) {                 // цикл по локальному куску\n",
        "    local_sum += (long long)local[i];                 // суммируем локальные элементы\n",
        "  }                                                   // конец цикла\n",
        "\n",
        "  long long global_sum = 0;                           // переменная для глобальной суммы (будет значима на rank 0)\n",
        "  MPI_Reduce(                                         // собираем локальные суммы в одну глобальную\n",
        "    &local_sum,                                       // адрес локальной суммы\n",
        "    &global_sum,                                      // адрес глобальной суммы (на root)\n",
        "    1,                                                // число элементов (одна сумма)\n",
        "    MPI_LONG_LONG,                                    // тип данных (long long)\n",
        "    MPI_SUM,                                          // операция редукции (сумма)\n",
        "    0,                                                // root = 0\n",
        "    MPI_COMM_WORLD                                    // коммуникатор\n",
        "  );                                                  // конец Reduce\n",
        "\n",
        "  MPI_Barrier(MPI_COMM_WORLD);                        // барьер: ждём, чтобы все завершили вычисления\n",
        "  double t1 = MPI_Wtime();                            // стоп таймера MPI\n",
        "\n",
        "  if (rank == 0) {                                    // если главный процесс\n",
        "    double ms = (t1 - t0) * 1000.0;                   // переводим секунды в миллисекунды\n",
        "    printf(\"MPI processes: %d | N=%d | sum=%lld | time=%.3f ms\\n\", size, N, global_sum, ms); // печать результатов\n",
        "  }                                                   // конец if (rank==0)\n",
        "\n",
        "  MPI_Finalize();                                     // завершаем работу MPI\n",
        "  return 0;                                           // выход из программы\n",
        "}                                                     // конец main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UuYgxFMtbEf",
        "outputId": "96e8ea47-0893-446a-db6b-a0dda995df16"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task4.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpicxx -O3 task4.cpp -o task4"
      ],
      "metadata": {
        "id": "94Oaw0JItqkG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --use-hwthread-cpus --oversubscribe -np 2 ./task4\n",
        "!mpirun --allow-run-as-root --use-hwthread-cpus --oversubscribe -np 4 ./task4\n",
        "!mpirun --allow-run-as-root --use-hwthread-cpus --oversubscribe -np 8 ./task4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFLCQY6xt6UG",
        "outputId": "7b715294-d916-44e2-d6f4-ebd713c5b670"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPI processes: 2 | N=10000000 | sum=45004663 | time=11.065 ms\n",
            "MPI processes: 4 | N=10000000 | sum=45004663 | time=12.384 ms\n",
            "MPI processes: 8 | N=10000000 | sum=45004663 | time=17.198 ms\n"
          ]
        }
      ]
    }
  ]
}