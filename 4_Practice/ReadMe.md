**Ответы на контрольные вопросы:**

**1. Чем отличаются типы памяти в CUDA и в каких случаях их использовать?**
В CUDA используется иерархия памяти, которая отличается скоростью и областью видимости. Самой быстрой являются регистры — они доступны только одному потоку и используются для локальных переменных и временных вычислений. Если регистров не хватает, данные попадают в локальную память, которая физически находится в глобальной и работает медленно. Разделяемая память доступна всем потокам одного блока и значительно быстрее глобальной, поэтому её используют для кэширования данных, тайлинга и коллективных операций внутри блока. Глобальная память имеет большой объём и доступна всем потокам, но обладает высокой задержкой, поэтому применяется в основном для хранения входных и выходных массивов. Константная и текстурная память оптимизированы для чтения: константная хорошо подходит для общих параметров, а текстурная — для нерегулярных шаблонов доступа.

**2. Как использование разделяемой памяти влияет на производительность?**
Использование разделяемой памяти обычно повышает производительность CUDA-программ, поскольку позволяет существенно сократить число обращений к медленной глобальной памяти. Потоки внутри одного блока могут многократно использовать одни и те же данные, загружая их один раз в shared memory. Это особенно эффективно для алгоритмов редукции, сортировки и матричных вычислений. Однако чрезмерное использование разделяемой памяти может снизить количество одновременно выполняемых блоков на мультипроцессоре, что уменьшает occupancy и в некоторых случаях нивелирует выигрыш.

**3. Что такое coalesced-доступ и как его обеспечить?**
Coalesced-доступ — это способ обращения к глобальной памяти, при котором потоки одного warp читают или записывают соседние адреса памяти. В таком случае GPU объединяет обращения в небольшое число транзакций, что значительно ускоряет доступ. Чтобы обеспечить coalesced-доступ, данные следует хранить последовательно в памяти, а индексацию строить так, чтобы поток с номером tid обращался к элементу массива с индексом, зависящим напрямую от tid, без больших шагов и разрывов.

**4. Какие сложности возникают при работе с большим объемом данных на GPU?**
При работе с большими объёмами данных основной проблемой становится ограниченный объём видеопамяти, из-за чего данные могут не помещаться целиком на GPU. Кроме того, передача данных между CPU и GPU является относительно медленной и может стать узким местом при частых копированиях. Даже при размещении данных в памяти GPU пропускная способность глобальной памяти может ограничивать производительность. В результате часто приходится разбивать данные на части и обрабатывать их поэтапно, что усложняет реализацию и синхронизацию.

**5. Почему важно минимизировать доступ к глобальной памяти?**
Глобальная память обладает высокой задержкой по сравнению с регистрами и разделяемой памятью, поэтому частые обращения к ней значительно замедляют выполнение kernel’ов. Минимизация доступа к глобальной памяти позволяет уменьшить задержки и повысить общую пропускную способность вычислений. Для этого данные стараются переиспользовать, кэшировать в разделяемой памяти и организовывать обращения таким образом, чтобы они были coalesced.

**6. Как использовать профилирование для анализа производительности CUDA-программ?**
Профилирование позволяет определить, какие части CUDA-программы являются узкими местами. Обычно сначала измеряется время выполнения kernel’ов с помощью CUDA-событий, чтобы получить базовую оценку. Затем используются инструменты NVIDIA Nsight для анализа взаимодействия CPU и GPU, времени копирования данных и последовательности выполнения kernel’ов. Более детальный анализ отдельных kernel’ов помогает выявить проблемы с доступом к памяти, низкую occupancy, расхождение ветвей и неэффективное использование ресурсов, после чего можно целенаправленно оптимизировать код.
