{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 1: Реализация обработки массива на CPU с использованием OpenMP**\n",
        "1. Создайте массив данных размером `N` (например, `N = 1 000 000`).\n",
        "2. Реализуйте функцию для обработки массива на CPU с использованием\n",
        "OpenMP. Например, умножьте каждый элемент массива на 2.\n",
        "3. Замерьте время выполнения обработки на CPU."
      ],
      "metadata": {
        "id": "WvdF-QqvCsd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Описание кода:\n",
        "* реализована обработка массива на CPU с OpenMP\n",
        "\n",
        "* реализована обработка массива на GPU с CUDA\n",
        "\n",
        "* реализован гибридный подход\n",
        "\n",
        "* проведены замеры времени"
      ],
      "metadata": {
        "id": "MSTlPzs-EEo0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsKMpW_6Cc_J",
        "outputId": "64394d30-3f15-49df-9ddf-08e45c3eb6e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile task1.cu\n",
        "#include <cstdio>                                                           // printf\n",
        "#include <vector>                                                           // std::vector\n",
        "#include <random>                                                           // генератор случайных чисел\n",
        "#include <chrono>                                                           // таймер CPU\n",
        "\n",
        "#ifdef _OPENMP                                                               // если OpenMP доступен\n",
        "#include <omp.h>                                                             // функции OpenMP\n",
        "#endif\n",
        "\n",
        "static void fill_random(std::vector<int>& a, int seed = 123)                 // Заполнение массива случайными числами\n",
        "{                                                                            // начало функции\n",
        "    std::mt19937 rng(seed);                                                  // генератор\n",
        "    std::uniform_int_distribution<int> dist(0, 9);                           // значения 0..9\n",
        "    for (size_t i = 0; i < a.size(); ++i) a[i] = dist(rng);                  // заполняем\n",
        "}                                                                            // конец функции\n",
        "\n",
        "static void cpu_process_openmp(int* a, int n)                                // CPU обработка: умножить каждый элемент на 2\n",
        "{                                                                            // начало функции\n",
        "    #pragma omp parallel for schedule(static)                                // OpenMP параллельный цикл\n",
        "    for (int i = 0; i < n; ++i)                                              // цикл по элементам\n",
        "    {                                                                        // начало цикла\n",
        "        a[i] = a[i] * 2;                                                     // умножаем на 2\n",
        "    }                                                                        // конец цикла\n",
        "}                                                                            // конец функции\n",
        "\n",
        "int main()                                                                    // точка входа\n",
        "{                                                                             // начало main\n",
        "    int N = 1'000'000;                                                        // размер массива\n",
        "    std::vector<int> a(N);                                                    // массив данных\n",
        "    fill_random(a);                                                           // заполняем\n",
        "\n",
        "#ifdef _OPENMP\n",
        "    printf(\"OpenMP включен, потоков (макс): %d\\n\", omp_get_max_threads());     // печатаем число потоков\n",
        "#else\n",
        "    printf(\"OpenMP не включен (компилируй с -Xcompiler -fopenmp)\\n\");          // предупреждение\n",
        "#endif\n",
        "\n",
        "    auto t0 = std::chrono::high_resolution_clock::now();                      // старт таймера\n",
        "    cpu_process_openmp(a.data(), N);                                           // обработка массива\n",
        "    auto t1 = std::chrono::high_resolution_clock::now();                      // стоп таймера\n",
        "\n",
        "    double ms = std::chrono::duration<double, std::milli>(t1 - t0).count();   // время в мс\n",
        "    printf(\"CPU(OpenMP) N=%d | time=%.3f ms\\n\", N, ms);                        // вывод времени\n",
        "\n",
        "    // простая проверка (первые 5 элементов)\n",
        "    printf(\"Пример (первые 5): \");\n",
        "    for (int i = 0; i < 5; ++i) printf(\"%d \", a[i]);\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    return 0;                                                                  // успешный выход\n",
        "}                                                                             // конец main\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -std=c++17 -Xcompiler -fopenmp task1.cu -o task1\n",
        "!./task1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEpJBXZUF4lv",
        "outputId": "cda1035a-b280-40f6-b362-127907f9b6cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenMP включен, потоков (макс): 2\n",
            "CPU(OpenMP) N=1000000 | time=0.252 ms\n",
            "Пример (первые 5): 12 14 4 8 4 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Анализ результатов:**\n",
        "\n",
        "По результатам выполнения первого задания обработка массива размером 1 000 000 элементов на CPU с использованием OpenMP была выполнена корректно: каждый элемент был умножен на 2, что видно по примеру первых значений (они стали чётными). Время выполнения составило 0.252 мс при использовании 2 потоков, что показывает, что параллелизация на CPU уменьшает время обработки по сравнению с последовательным циклом, однако общий выигрыш ограничен небольшим числом доступных потоков и пропускной способностью памяти."
      ],
      "metadata": {
        "id": "A8Hdy7RrGr6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 2: Реализация обработки массива на GPU с использованием CUDA**\n",
        "1. Скопируйте массив данных на GPU.\n",
        "2. Реализуйте ядро CUDA для обработки массива на GPU. Например, умножьте\n",
        "каждый элемент массива на 2.\n",
        "3. Скопируйте обработанные данные обратно на CPU.\n",
        "4. Замерьте время выполнения обработки на GPU."
      ],
      "metadata": {
        "id": "q1AhKkaOCw6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task2.cu\n",
        "#include <cuda_runtime.h>                                                   // CUDA runtime\n",
        "#include <cstdio>                                                           // printf\n",
        "#include <vector>                                                           // std::vector\n",
        "#include <random>                                                           // rng\n",
        "#include <chrono>                                                           // CPU таймер (если нужно)\n",
        "#include <cstdlib>                                                          // exit\n",
        "\n",
        "static inline void cuda_check(cudaError_t err, const char* file, int line)  // проверка CUDA ошибок\n",
        "{\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        printf(\"Ошибка CUDA %s:%d: %s\\n\", file, line, cudaGetErrorString(err));\n",
        "        std::exit(1);\n",
        "    }\n",
        "}\n",
        "#define CHECK_CUDA(call) cuda_check((call), __FILE__, __LINE__)\n",
        "\n",
        "static void fill_random(std::vector<int>& a, int seed = 123)                // заполнение случайными\n",
        "{\n",
        "    std::mt19937 rng(seed);\n",
        "    std::uniform_int_distribution<int> dist(0, 9);\n",
        "    for (size_t i = 0; i < a.size(); ++i) a[i] = dist(rng);\n",
        "}\n",
        "\n",
        "__global__ void gpu_process_kernel(int* a, int n)                           // ядро: умножить на 2\n",
        "{\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (gid < n) a[gid] = a[gid] * 2;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int N = 1'000'000;                                                     // размер массива\n",
        "    int threads = 256;                                                     // потоки в блоке\n",
        "    int blocks = (N + threads - 1) / threads;                              // блоки\n",
        "\n",
        "    std::vector<int> h(N);                                                 // массив на CPU\n",
        "    fill_random(h);                                                        // заполняем\n",
        "\n",
        "    int* d = nullptr;                                                      // указатель на GPU\n",
        "    CHECK_CUDA(cudaMalloc(&d, N * sizeof(int)));                            // выделяем на GPU\n",
        "\n",
        "    cudaEvent_t start_total, stop_total, start_k, stop_k;                   // события для времени\n",
        "    CHECK_CUDA(cudaEventCreate(&start_total));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop_total));\n",
        "    CHECK_CUDA(cudaEventCreate(&start_k));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop_k));\n",
        "\n",
        "    CHECK_CUDA(cudaEventRecord(start_total));                               // старт total\n",
        "\n",
        "    CHECK_CUDA(cudaMemcpy(d, h.data(), N * sizeof(int), cudaMemcpyHostToDevice)); // H2D\n",
        "\n",
        "    CHECK_CUDA(cudaEventRecord(start_k));                                   // старт kernel\n",
        "    gpu_process_kernel<<<blocks, threads>>>(d, N);                          // запуск ядра\n",
        "    CHECK_CUDA(cudaGetLastError());                                         // проверка запуска\n",
        "    CHECK_CUDA(cudaEventRecord(stop_k));                                    // стоп kernel\n",
        "\n",
        "    CHECK_CUDA(cudaMemcpy(h.data(), d, N * sizeof(int), cudaMemcpyDeviceToHost)); // D2H\n",
        "\n",
        "    CHECK_CUDA(cudaEventRecord(stop_total));                                // стоп total\n",
        "    CHECK_CUDA(cudaEventSynchronize(stop_total));                           // ждём\n",
        "\n",
        "    float kernel_ms = 0.0f, total_ms = 0.0f;                                // времена\n",
        "    CHECK_CUDA(cudaEventElapsedTime(&kernel_ms, start_k, stop_k));          // kernel time\n",
        "    CHECK_CUDA(cudaEventElapsedTime(&total_ms, start_total, stop_total));   // total time\n",
        "\n",
        "    printf(\"GPU(CUDA) N=%d | kernel=%.3f ms | total(H2D+kernel+D2H)=%.3f ms\\n\", N, kernel_ms, total_ms);\n",
        "\n",
        "    printf(\"Пример (первые 5): \");\n",
        "    for (int i = 0; i < 5; ++i) printf(\"%d \", h[i]);\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start_total));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop_total));\n",
        "    CHECK_CUDA(cudaEventDestroy(start_k));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop_k));\n",
        "    CHECK_CUDA(cudaFree(d));\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6ZIN5bMCxPH",
        "outputId": "cae9ec7d-de9e-4cfd-936e-9e64d9104c42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -std=c++17 task2.cu -o task2 -gencode arch=compute_75,code=sm_75\n",
        "!./task2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE0Jn5xfDAhS",
        "outputId": "4cd89c4f-5d46-410d-aca6-38b7b2116fd5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU(CUDA) N=1000000 | kernel=0.073 ms | total(H2D+kernel+D2H)=2.062 ms\n",
            "Пример (первые 5): 12 14 4 8 4 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Анализ результатов:**\n",
        "\n",
        "По результатам второго задания обработка массива на GPU выполнена корректно, что подтверждается совпадением примера первых элементов с ожидаемым результатом (значения также стали чётными после умножения на 2). Время выполнения ядра составило 0.073 мс, то есть сама вычислительная часть на GPU заметно быстрее CPU-обработки из первого задания. Однако общее время выполнения 2.062 мс оказалось значительно больше kernel-времени, потому что сюда входят накладные расходы на копирование данных между CPU и GPU (H2D и D2H) и синхронизация. Это показывает, что при единичной операции “умножить на 2” главным ограничением становится не вычисление, а передача данных."
      ],
      "metadata": {
        "id": "knT4isDkG09S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 3: Гибридная обработка массива**\n",
        "1. Разделите массив на две части: первая половина обрабатывается на CPU,\n",
        "вторая — на GPU.\n",
        "2. Реализуйте гибридное приложение, которое выполняет обработку массива\n",
        "на CPU и GPU одновременно.\n",
        "3. Замерьте общее время выполнения гибридной обработки."
      ],
      "metadata": {
        "id": "gptuDQalCxj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task3.cu\n",
        "#include <cuda_runtime.h>                                                   // CUDA runtime\n",
        "#include <cstdio>                                                           // printf\n",
        "#include <cstdlib>                                                          // exit\n",
        "#include <random>                                                           // rng\n",
        "#include <chrono>                                                           // таймер\n",
        "#include <thread>                                                           // std::thread\n",
        "\n",
        "#ifdef _OPENMP\n",
        "#include <omp.h>\n",
        "#endif\n",
        "\n",
        "static inline void cuda_check(cudaError_t err, const char* file, int line)  // проверка CUDA\n",
        "{\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        printf(\"Ошибка CUDA %s:%d: %s\\n\", file, line, cudaGetErrorString(err));\n",
        "        std::exit(1);\n",
        "    }\n",
        "}\n",
        "#define CHECK_CUDA(call) cuda_check((call), __FILE__, __LINE__)\n",
        "\n",
        "__global__ void gpu_process_kernel(int* a, int n)                           // ядро: умножение на 2\n",
        "{\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (gid < n) a[gid] = a[gid] * 2;\n",
        "}\n",
        "\n",
        "static void cpu_process_openmp(int* a, int n)                               // CPU обработка (OpenMP)\n",
        "{\n",
        "    #pragma omp parallel for schedule(static)\n",
        "    for (int i = 0; i < n; ++i) a[i] = a[i] * 2;\n",
        "}\n",
        "\n",
        "static void fill_random(int* a, int n, int seed = 123)                      // заполнение массива\n",
        "{\n",
        "    std::mt19937 rng(seed);\n",
        "    std::uniform_int_distribution<int> dist(0, 9);\n",
        "    for (int i = 0; i < n; ++i) a[i] = dist(rng);\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int N = 1'000'000;                                                     // размер массива\n",
        "    int threads = 256;                                                     // потоки CUDA\n",
        "\n",
        "#ifdef _OPENMP\n",
        "    printf(\"OpenMP включен, потоков (макс): %d\\n\", omp_get_max_threads());\n",
        "#else\n",
        "    printf(\"OpenMP не включен (компилируй с -Xcompiler -fopenmp)\\n\");\n",
        "#endif\n",
        "\n",
        "    // Важно: используем pinned память, чтобы cudaMemcpyAsync была реально асинхронной\n",
        "    int* h = nullptr;                                                      // массив на CPU (pinned)\n",
        "    CHECK_CUDA(cudaHostAlloc(&h, N * sizeof(int), cudaHostAllocDefault));   // выделяем pinned\n",
        "\n",
        "    fill_random(h, N, 123);                                                // заполняем\n",
        "\n",
        "    int n1 = N / 2;                                                        // первая половина\n",
        "    int n2 = N - n1;                                                       // вторая половина\n",
        "    int offset = n1;                                                       // смещение второй половины\n",
        "\n",
        "    cudaStream_t stream;                                                   // CUDA stream\n",
        "    CHECK_CUDA(cudaStreamCreate(&stream));                                 // создаём stream\n",
        "\n",
        "    auto t0 = std::chrono::high_resolution_clock::now();                   // старт общего времени\n",
        "\n",
        "    float gpu_kernel_ms = 0.0f;                                            // kernel время GPU половины (для справки)\n",
        "\n",
        "    // GPU часть запускаем в отдельном потоке CPU\n",
        "    std::thread gpu_thread([&]() {\n",
        "        int* d = nullptr;                                                  // память под половину на GPU\n",
        "        CHECK_CUDA(cudaMalloc(&d, n2 * sizeof(int)));                       // выделяем\n",
        "\n",
        "        int blocks = (n2 + threads - 1) / threads;                          // блоки\n",
        "\n",
        "        cudaEvent_t ks, ke;                                                 // события kernel времени\n",
        "        CHECK_CUDA(cudaEventCreate(&ks));\n",
        "        CHECK_CUDA(cudaEventCreate(&ke));\n",
        "\n",
        "        CHECK_CUDA(cudaMemcpyAsync(d, h + offset, n2 * sizeof(int), cudaMemcpyHostToDevice, stream)); // H2D async\n",
        "\n",
        "        CHECK_CUDA(cudaEventRecord(ks, stream));                            // старт kernel\n",
        "        gpu_process_kernel<<<blocks, threads, 0, stream>>>(d, n2);          // ядро\n",
        "        CHECK_CUDA(cudaGetLastError());\n",
        "        CHECK_CUDA(cudaEventRecord(ke, stream));                            // стоп kernel\n",
        "\n",
        "        CHECK_CUDA(cudaMemcpyAsync(h + offset, d, n2 * sizeof(int), cudaMemcpyDeviceToHost, stream)); // D2H async\n",
        "        CHECK_CUDA(cudaStreamSynchronize(stream));                          // ждём stream\n",
        "\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&gpu_kernel_ms, ks, ke));           // kernel время\n",
        "\n",
        "        CHECK_CUDA(cudaEventDestroy(ks));\n",
        "        CHECK_CUDA(cudaEventDestroy(ke));\n",
        "        CHECK_CUDA(cudaFree(d));\n",
        "    });\n",
        "\n",
        "    // CPU часть выполняется одновременно\n",
        "    cpu_process_openmp(h, n1);                                              // обрабатываем первую половину\n",
        "\n",
        "    gpu_thread.join();                                                     // ждём GPU поток\n",
        "\n",
        "    auto t1 = std::chrono::high_resolution_clock::now();                   // стоп\n",
        "    double total_ms = std::chrono::duration<double, std::milli>(t1 - t0).count();\n",
        "\n",
        "    printf(\"HYBRID N=%d | total=%.3f ms | gpu_half_kernel=%.3f ms (справочно)\\n\", N, total_ms, gpu_kernel_ms);\n",
        "\n",
        "    printf(\"Пример (первые 5): \");\n",
        "    for (int i = 0; i < 5; ++i) printf(\"%d \", h[i]);\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    CHECK_CUDA(cudaStreamDestroy(stream));\n",
        "    CHECK_CUDA(cudaFreeHost(h));\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf_1k0wOCx68",
        "outputId": "33ab02db-a6c2-47d1-e654-8a308202af07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -std=c++17 -Xcompiler -fopenmp task3.cu -o task3 -gencode arch=compute_75,code=sm_75\n",
        "!./task3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdp42zRBHMx4",
        "outputId": "7a654f2a-6c84-41e5-b89c-2fc16d5d5d6d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenMP включен, потоков (макс): 2\n",
            "HYBRID N=1000000 | total=0.951 ms | gpu_half_kernel=0.060 ms (справочно)\n",
            "Пример (первые 5): 12 14 4 8 4 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Анализ результатов:**\n",
        "\n",
        "По результатам третьего задания гибридная обработка массива размером 1 000 000 элементов выполнена корректно, что подтверждается примером первых значений после умножения на 2. Общее время гибридного режима составило 0.951 мс, при этом kernel-время GPU для второй половины массива было всего 0.060 мс, то есть вычисления на GPU выполняются быстро. Однако итоговое время гибрида оказалось выше, чем у чистого CPU в первом задании (0.252 мс), поскольку в гибридном режиме сохраняются накладные расходы на передачу данных между CPU и GPU и дополнительная стоимость организации параллельного запуска (stream, асинхронные копирования, поток). Это показывает, что при простой операции и относительно небольшом числе потоков OpenMP гибридный подход не даёт выигрыша и становится выгодным только при более тяжёлых вычислениях или при повторном использовании данных на GPU без постоянных копирований."
      ],
      "metadata": {
        "id": "FSXKZhDUH2d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 4: Анализ производительности**\n",
        "1. Сравните время выполнения обработки массива на CPU, GPU и в гибридном\n",
        "режиме.\n",
        "2. Проведите анализ производительности и определите, в каких случаях\n",
        "гибридный подход дает наибольший выигрыш."
      ],
      "metadata": {
        "id": "2vK5VWdLCyO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task4.cu\n",
        "#include <cuda_runtime.h>                 // Подключаем CUDA Runtime API для работы с GPU\n",
        "#include <cstdio>                         // Подключаем printf для вывода в консоль\n",
        "#include <cstdlib>                        // Подключаем std::exit для аварийного завершения программы\n",
        "#include <vector>                         // Подключаем std::vector для хранения массивов на CPU\n",
        "#include <random>                         // Подключаем генератор случайных чисел\n",
        "#include <chrono>                         // Подключаем средства измерения времени\n",
        "#include <thread>                         // Подключаем std::thread для гибридного режима\n",
        "\n",
        "#ifdef _OPENMP\n",
        "#include <omp.h>                          // Подключаем OpenMP, если он поддерживается\n",
        "#endif\n",
        "\n",
        "static inline void cuda_check(cudaError_t err, const char* file, int line)\n",
        "{\n",
        "    if (err != cudaSuccess)               // Если произошла ошибка CUDA\n",
        "    {\n",
        "        printf(\"Ошибка CUDA %s:%d: %s\\n\", // Выводим имя файла, номер строки и описание ошибки\n",
        "               file, line, cudaGetErrorString(err));\n",
        "        std::exit(1);                     // Завершаем программу с ошибкой\n",
        "    }\n",
        "}\n",
        "\n",
        "#define CHECK_CUDA(call) cuda_check((call), __FILE__, __LINE__) // Макрос для удобной проверки CUDA-вызовов\n",
        "\n",
        "static void fill_random(std::vector<int>& a, int seed = 123)\n",
        "{\n",
        "    std::mt19937 rng(seed);               // Инициализируем генератор случайных чисел\n",
        "    std::uniform_int_distribution<int> dist(0, 9); // Диапазон случайных чисел от 0 до 9\n",
        "    for (size_t i = 0; i < a.size(); ++i) // Проходим по всему массиву\n",
        "        a[i] = dist(rng);                 // Записываем случайное значение в элемент массива\n",
        "}\n",
        "\n",
        "static void cpu_process_openmp(int* a, int n)\n",
        "{\n",
        "    #pragma omp parallel for schedule(static) // Параллельный цикл OpenMP\n",
        "    for (int i = 0; i < n; ++i)              // Проходим по всем элементам массива\n",
        "        a[i] *= 2;                           // Умножаем каждый элемент на 2\n",
        "}\n",
        "\n",
        "__global__ void gpu_process_kernel(int* a, int n)\n",
        "{\n",
        "    int gid = blockIdx.x * blockDim.x + threadIdx.x; // Вычисляем глобальный индекс потока\n",
        "    if (gid < n)                                    // Проверяем выход за границы массива\n",
        "        a[gid] *= 2;                               // Умножаем элемент массива на 2\n",
        "}\n",
        "\n",
        "static float gpu_total_ms(int* h, int n, int threads)\n",
        "{\n",
        "    int blocks = (n + threads - 1) / threads; // Вычисляем количество блоков CUDA\n",
        "\n",
        "    int* d = nullptr;                          // Указатель на массив в памяти GPU\n",
        "    CHECK_CUDA(cudaMalloc(&d, n * sizeof(int))); // Выделяем память на GPU\n",
        "\n",
        "    cudaEvent_t s, e;                          // CUDA-события для замера времени\n",
        "    CHECK_CUDA(cudaEventCreate(&s));           // Создаем событие начала\n",
        "    CHECK_CUDA(cudaEventCreate(&e));           // Создаем событие конца\n",
        "\n",
        "    CHECK_CUDA(cudaEventRecord(s));            // Фиксируем начало общего времени\n",
        "\n",
        "    CHECK_CUDA(cudaMemcpy(d, h, n * sizeof(int),\n",
        "                           cudaMemcpyHostToDevice)); // Копируем данные с CPU на GPU\n",
        "\n",
        "    gpu_process_kernel<<<blocks, threads>>>(d, n);   // Запускаем CUDA-ядро\n",
        "    CHECK_CUDA(cudaGetLastError());                   // Проверяем корректность запуска ядра\n",
        "\n",
        "    CHECK_CUDA(cudaMemcpy(h, d, n * sizeof(int),\n",
        "                           cudaMemcpyDeviceToHost)); // Копируем результат обратно на CPU\n",
        "\n",
        "    CHECK_CUDA(cudaEventRecord(e));            // Фиксируем конец общего времени\n",
        "    CHECK_CUDA(cudaEventSynchronize(e));       // Ждём завершения всех операций\n",
        "\n",
        "    float ms = 0.0f;                           // Переменная для хранения времени\n",
        "    CHECK_CUDA(cudaEventElapsedTime(&ms, s, e)); // Вычисляем общее время выполнения\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(s));           // Удаляем событие начала\n",
        "    CHECK_CUDA(cudaEventDestroy(e));           // Удаляем событие конца\n",
        "    CHECK_CUDA(cudaFree(d));                   // Освобождаем память GPU\n",
        "\n",
        "    return ms;                                 // Возвращаем общее время GPU\n",
        "}\n",
        "\n",
        "static double hybrid_total_ms(int* pinned, int n, int threads)\n",
        "{\n",
        "    int n1 = n / 2;                            // Размер первой половины массива\n",
        "    int n2 = n - n1;                           // Размер второй половины массива\n",
        "    int offset = n1;                           // Смещение для второй половины\n",
        "\n",
        "    cudaStream_t stream;                       // CUDA stream для асинхронных операций\n",
        "    CHECK_CUDA(cudaStreamCreate(&stream));     // Создаем stream\n",
        "\n",
        "    auto t0 = std::chrono::high_resolution_clock::now(); // Старт общего таймера\n",
        "\n",
        "    std::thread gpu_thread([&]()               // Запускаем GPU-часть в отдельном CPU-потоке\n",
        "    {\n",
        "        int* d = nullptr;                      // Указатель на память GPU\n",
        "        CHECK_CUDA(cudaMalloc(&d, n2 * sizeof(int))); // Выделяем память под вторую половину\n",
        "\n",
        "        int blocks = (n2 + threads - 1) / threads; // Количество блоков CUDA\n",
        "\n",
        "        CHECK_CUDA(cudaMemcpyAsync(d, pinned + offset,\n",
        "                                   n2 * sizeof(int),\n",
        "                                   cudaMemcpyHostToDevice, stream)); // Асинхронная копия на GPU\n",
        "\n",
        "        gpu_process_kernel<<<blocks, threads, 0, stream>>>(d, n2); // Запуск ядра в stream\n",
        "        CHECK_CUDA(cudaGetLastError());                             // Проверка ядра\n",
        "\n",
        "        CHECK_CUDA(cudaMemcpyAsync(pinned + offset, d,\n",
        "                                   n2 * sizeof(int),\n",
        "                                   cudaMemcpyDeviceToHost, stream)); // Асинхронная копия обратно\n",
        "\n",
        "        CHECK_CUDA(cudaStreamSynchronize(stream)); // Ждём завершения GPU-операций\n",
        "        CHECK_CUDA(cudaFree(d));                   // Освобождаем память GPU\n",
        "    });\n",
        "\n",
        "    cpu_process_openmp(pinned, n1);               // CPU обрабатывает первую половину массива\n",
        "\n",
        "    gpu_thread.join();                            // Ждём завершения GPU-потока\n",
        "\n",
        "    auto t1 = std::chrono::high_resolution_clock::now(); // Останавливаем таймер\n",
        "    double ms = std::chrono::duration<double, std::milli>(t1 - t0).count(); // Считаем время\n",
        "\n",
        "    CHECK_CUDA(cudaStreamDestroy(stream));        // Удаляем CUDA stream\n",
        "    return ms;                                    // Возвращаем общее время гибридной обработки\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int N = 1'000'000;                            // Размер массива\n",
        "    int threads = 256;                            // Количество потоков CUDA в блоке\n",
        "\n",
        "#ifdef _OPENMP\n",
        "    printf(\"OpenMP включен, потоков (макс): %d\\n\",\n",
        "           omp_get_max_threads());                // Вывод количества потоков OpenMP\n",
        "#else\n",
        "    printf(\"OpenMP не включен\\n\");                // Сообщение, если OpenMP недоступен\n",
        "#endif\n",
        "\n",
        "    printf(\"N=%d\\n\\n\", N);                        // Печать размера массива\n",
        "\n",
        "    std::vector<int> base(N);                     // Исходный массив\n",
        "    fill_random(base);                            // Заполняем его случайными числами\n",
        "\n",
        "    std::vector<int> cpu = base;                  // Копия массива для CPU\n",
        "    auto c0 = std::chrono::high_resolution_clock::now(); // Старт CPU таймера\n",
        "    cpu_process_openmp(cpu.data(), N);            // Обработка массива на CPU\n",
        "    auto c1 = std::chrono::high_resolution_clock::now(); // Стоп CPU таймера\n",
        "    double cpu_ms = std::chrono::duration<double, std::milli>(c1 - c0).count(); // Время CPU\n",
        "\n",
        "    std::vector<int> gpu = base;                  // Копия массива для GPU\n",
        "    float gpu_ms = gpu_total_ms(gpu.data(), N, threads); // GPU общее время\n",
        "\n",
        "    int* pinned = nullptr;                        // Указатель на pinned-память\n",
        "    CHECK_CUDA(cudaHostAlloc(&pinned,\n",
        "                             N * sizeof(int),\n",
        "                             cudaHostAllocDefault)); // Выделяем pinned-память\n",
        "    for (int i = 0; i < N; ++i) pinned[i] = base[i]; // Копируем данные\n",
        "\n",
        "    double hybrid_ms = hybrid_total_ms(pinned, N, threads); // Гибридная обработка\n",
        "    CHECK_CUDA(cudaFreeHost(pinned));        // Освобождаем pinned-память\n",
        "\n",
        "    printf(\"CPU(OpenMP): %.3f ms\\n\", cpu_ms);       // Вывод времени CPU\n",
        "    printf(\"GPU(total):  %.3f ms\\n\", gpu_ms);       // Вывод времени GPU\n",
        "    printf(\"HYBRID:      %.3f ms\\n\\n\", hybrid_ms);  // Вывод времени гибрида\n",
        "\n",
        "    printf(\"Ускорение GPU(total) относительно CPU: %.2f x\\n\",\n",
        "           cpu_ms / gpu_ms);                         // Коэффициент ускорения GPU\n",
        "    printf(\"Ускорение HYBRID относительно CPU:     %.2f x\\n\",\n",
        "           cpu_ms / hybrid_ms);                      // Коэффициент ускорения гибрида\n",
        "\n",
        "    return 0;                                        // Завершение программы\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "098Rr59QC_R5",
        "outputId": "b6f8f859-704f-4fc1-b932-3af48218377c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task4.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -std=c++17 -Xcompiler -fopenmp task4.cu -o task4 -gencode arch=compute_75,code=sm_75\n",
        "!./task4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR3MUVxoDBYv",
        "outputId": "6c701028-0e2d-40db-f825-eccb89c40cc5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenMP включен, потоков (макс): 2\n",
            "N=1000000\n",
            "\n",
            "CPU(OpenMP): 0.302 ms\n",
            "GPU(total):  2.103 ms\n",
            "HYBRID:      0.896 ms\n",
            "\n",
            "Ускорение GPU(total) относительно CPU: 0.14 x\n",
            "Ускорение HYBRID относительно CPU:     0.34 x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Анализ результатов:**\n",
        "\n",
        "По результатам сравнения в четвертом задании видно, что при размере массива 1 000 000 элементов быстрее всего выполняется обработка на CPU с OpenMP (0.302 мс). Несмотря на то, что вычисления на GPU сами по себе выполняются очень быстро, общее время GPU-режима оказалось значительно больше (2.103 мс), потому что основную долю занимают копирования данных между хостом и устройством и синхронизация, а сама операция умножения на 2 слишком простая и не успевает “окупать” эти накладные расходы. Гибридный режим показал промежуточный результат (0.896 мс): часть работы действительно выполняется параллельно на CPU и GPU, но затраты на передачу второй половины массива на GPU и возврат результата всё равно остаются, поэтому суммарно он проигрывает чистому CPU. Полученные коэффициенты ускорения меньше единицы (GPU: 0.14×, Hybrid: 0.34×), что означает замедление относительно CPU и подтверждает, что для данной операции и одного прохода по данным наиболее эффективен CPU, а GPU/гибрид становятся выгодными только при более вычислительно сложной обработке или при сценариях, где данные долго остаются на GPU и копирования выполняются редко."
      ],
      "metadata": {
        "id": "nOTDFuJoIOmD"
      }
    }
  ]
}