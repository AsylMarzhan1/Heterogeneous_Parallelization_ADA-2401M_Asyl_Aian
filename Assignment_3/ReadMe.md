**Ответы на контрольные вопросы:**

**1.Какие основные типы памяти существуют в архитектуре CUDA и чем они
отличаются по скорости доступа?**

В CUDA есть несколько уровней памяти, которые сильно отличаются по скорости, объёму и месту хранения.

Самая быстрая — это регистры: они находятся прямо внутри каждого SM (Streaming Multiprocessor) и доступны потоку практически мгновенно. Но регистров мало, и если переменных слишком много, часть данных «вытесняется» в более медленную память.

Далее идёт разделяемая память: она тоже находится на SM и общая для всех потоков одного блока. По скорости она близка к регистрам, но медленнее, чем регистры, зато её удобно использовать для обмена данными между потоками блока.

Затем — глобальная память: это основная память GPU (DRAM). Она самая большая по объёму, но доступ к ней заметно медленнее, потому что это внешняя память относительно SM.

Также существуют локальная память — это не отдельная физическая память, а часть глобальной памяти, которая используется, если потоку не хватает регистров. Она медленная и обычно ухудшает производительность.

Ещё есть константная и текстурная память: они используют кэширование и хорошо работают в специфических сценариях доступа (например, когда все потоки читают одно и то же значение).

**2.В каких случаях использование разделяемой памяти позволяет ускорить
выполнение CUDA-программы?**

Разделяемая память ускоряет программу тогда, когда потоки одного блока повторно используют одни и те же данные, либо когда нужно организовать быстрый обмен информацией между потоками.

Классический пример — умножение матриц: блок потоков загружает кусок матрицы в shared memory (tile), и затем множество потоков много раз использует эти данные, не обращаясь каждый раз в global memory.

Также shared memory помогает, когда глобальная память читается «неудобно» (разрозненно), а данные можно сначала собрать/переложить в shared memory и потом работать быстрее и более упорядоченно.

**3.Как шаблон доступа к глобальной памяти влияет на производительность GPU программы?**

GPU работает наиболее эффективно, когда доступ к глобальной памяти является coalesced (согласованным). Это означает, что потоки одного варпа читают данные, лежащие рядом в памяти, и тогда GPU может обслужить их запросы минимальным количеством транзакций памяти.

Если же потоки читают память «вразнобой» (например, через большие шаги или по случайным индексам), то возникает много отдельных транзакций, и производительность резко падает, потому что глобальная память медленная и её задержка становится bottleneck.

То есть один и тот же объём данных можно прочитать либо эффективно (несколькими транзакциями), либо крайне неэффективно (десятками транзакций).

**4.Почему одинаковый алгоритм на GPU может показывать разное время выполнения
при разных способах обращения к памяти?**

Потому что время выполнения на GPU очень часто определяется не вычислениями, а именно тем, сколько времени занимает доставка данных.

Если алгоритм делает одинаковое количество операций, но в одном случае он читает память coalesced и использует shared memory, а в другом читает глобальную память хаотично и часто, то итоговое время может отличаться в разы.

Также играет роль, насколько хорошо попадают данные в кэш, сколько происходит конфликтов в shared memory (bank conflicts), и не происходит ли регистровое переполнение с уходом в local memory.

**5.Как размер блока потоков влияет на производительность CUDA-ядра?**

Размер блока влияет на то, насколько хорошо GPU сможет «заполнить» вычислительные блоки SM потоками.

Если блок слишком маленький, то может быть низкая загрузка SM и плохое скрытие задержек памяти. Если блок слишком большой, то могут закончиться ресурсы SM — например, регистры или shared memory — и тогда одновременно на SM будет запускаться меньше блоков, что тоже снижает производительность.

На практике удобные размеры блока чаще всего кратны 32 (то есть 128, 256, 512 потоков), потому что выполнение идёт варпами.

**6.Что такое варп и почему важно учитывать его при разработке CUDA-программ?**

Варп — это группа из 32 потоков, которая исполняется на GPU синхронно по принципу SIMD/SIMT: фактически одна инструкция применяется сразу к 32 потокам.

Учитывать warp важно по двум причинам.

Во-первых, если внутри варпа возникает ветвление (if), и часть потоков идёт в одну ветку, а часть — в другую, GPU вынужден выполнять обе ветки по очереди. Это называется warp divergence, и оно снижает производительность.

Во-вторых, шаблон доступа памяти должен быть согласован именно на уровне варпа: coalescing рассчитывается как раз по потокам одного варпа.

**7.Какие факторы необходимо учитывать при выборе конфигурации сетки и блоков
потоков?**

Нужно учитывать, сколько потоков реально нужно для задачи, а также ограничения GPU по ресурсам.

Главные факторы: размер данных и способ разбиения (1D/2D/3D), удобный размер блока (кратность 32), объём shared memory на блок, количество регистров на поток, ожидаемая occupancy (сколько варпов может быть активным на SM), а также характер вычислений — memory-bound или compute-bound.

Часто конфигурацию выбирают так, чтобы было достаточно потоков для полной загрузки GPU и чтобы не «задушить» SM чрезмерным потреблением shared memory/регистров.

**8.Почему оптимизация CUDA-программы часто начинается с анализа работы с
памятью, а не с изменения алгоритма?**

Потому что большинство CUDA-программ упираются в пропускную способность памяти, а не в вычисления.

Даже если алгоритм хороший, но постоянно обращается к global memory неэффективно, GPU будет простаивать, ожидая данные. Поэтому первое, что обычно проверяют — coalescing, повторное использование данных, возможность загрузки в shared memory, устранение лишних чтений/записей и уменьшение расходов памяти.

И только когда память используется эффективно, имеет смысл оптимизировать сам алгоритм: распараллеливание вычислений, уменьшение операций, улучшение численных процедур и т.д.
