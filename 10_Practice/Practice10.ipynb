{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 1.** Анализ производительности CPU-параллельной программы (OpenMP)\n",
        "Разработайте параллельную программу на C++ с использованием OpenMP для обработки\n",
        "большого массива данных (например, вычисление суммы, среднего значения и\n",
        "дисперсии).\n",
        "\n",
        "**Требуется:**\n",
        "* реализовать базовую параллельную версию;\n",
        "* выполнить профилирование программы с использованием omp_get_wtime() и/или\n",
        "профилировщика (Intel VTune, gprof);\n",
        "\n",
        "**определить:**\n",
        "* долю параллельной и последовательной части программы;\n",
        "* влияние числа потоков на ускорение;\n",
        "* проанализировать результаты в контексте закона Амдала."
      ],
      "metadata": {
        "id": "6JzBVer_RRYB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vpCp6jd6RN3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd7923e-6794-4737-d936-b89a0ad1900e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting omp_task1.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile omp_task1.cpp\n",
        "\n",
        "#include <omp.h>                 // OpenMP (omp_get_wtime, omp_set_num_threads)\n",
        "#include <iostream>              // cout\n",
        "#include <vector>                // vector\n",
        "#include <random>                // mt19937, uniform\n",
        "#include <cmath>                 // fabs\n",
        "#include <cstdlib>               // atoll\n",
        "\n",
        "int main(int argc, char** argv)                                          // вход\n",
        "{\n",
        "    long long N = 50'000'000;                                            // размер массива по умолчанию (50 млн)\n",
        "    if (argc > 1) N = std::atoll(argv[1]);                               // если передали N\n",
        "\n",
        "    if (N <= 0)                                                          // проверка\n",
        "    {\n",
        "        std::cout << \"N должно быть > 0\\n\";                              // сообщение\n",
        "        return 0;                                                        // выход\n",
        "    }\n",
        "\n",
        "    std::vector<double> a;                                               // массив данных\n",
        "    a.resize((size_t)N);                                                 // выделяем память\n",
        "\n",
        "    std::mt19937 gen(42);                                                // генератор случайных чисел\n",
        "    std::uniform_real_distribution<double> dist(0.0, 1.0);               // распределение 0..1\n",
        "\n",
        "    for (long long i = 0; i < N; ++i)                                    // заполняем массив (последовательно)\n",
        "        a[(size_t)i] = dist(gen);                                        // записали число\n",
        "\n",
        "    int max_threads = omp_get_max_threads();                             // сколько потоков доступно\n",
        "    std::cout << \"N = \" << N << \"\\n\";                                    // печать N\n",
        "    std::cout << \"Max threads available = \" << max_threads << \"\\n\\n\";    // печать max потоков\n",
        "\n",
        "    //  ПОСЛЕДОВАТЕЛЬНАЯ ВЕРСИЯ (baseline)\n",
        "    double t0_seq = omp_get_wtime();                                     // старт времени seq\n",
        "\n",
        "    double sum_seq = 0.0;                                                // сумма\n",
        "    for (long long i = 0; i < N; ++i)                                    // цикл\n",
        "        sum_seq += a[(size_t)i];                                         // суммирование\n",
        "\n",
        "    double mean_seq = sum_seq / (double)N;                               // среднее\n",
        "\n",
        "    double var_seq_acc = 0.0;                                            // накопитель дисперсии\n",
        "    for (long long i = 0; i < N; ++i)                                    // второй проход\n",
        "    {\n",
        "        double d = a[(size_t)i] - mean_seq;                              // отклонение\n",
        "        var_seq_acc += d * d;                                            // квадрат отклонения\n",
        "    }\n",
        "    double var_seq = var_seq_acc / (double)N;                            // дисперсия\n",
        "\n",
        "    double t1_seq = omp_get_wtime();                                     // конец времени seq\n",
        "    double time_seq = t1_seq - t0_seq;                                   // время seq\n",
        "\n",
        "    std::cout << \"[SEQ] sum=\" << sum_seq                                 // вывод суммы\n",
        "              << \" mean=\" << mean_seq                                    // вывод среднего\n",
        "              << \" var=\" << var_seq                                      // вывод дисперсии\n",
        "              << \" time=\" << time_seq << \" s\\n\\n\";                       // вывод времени\n",
        "\n",
        "    //ПАРАЛЛЕЛЬНЫЕ ЗАПУСКИ ДЛЯ РАЗНЫХ ЧИСЕЛ ПОТОКОВ\n",
        "    // Будем прогонять 1,2,4,8,... до max_threads (степени двойки)\n",
        "    std::cout << \"threads,time_parallel,speedup,efficiency,parallel_fraction(f),serial_fraction(1-f)\\n\";\n",
        "\n",
        "    for (int threads = 1; threads <= max_threads; threads *= 2)          // перебор потоков\n",
        "    {\n",
        "        omp_set_num_threads(threads);                                    // задаём число потоков\n",
        "\n",
        "        // Чтобы сравнение было честным, считаем то же самое: sum -> mean -> variance\n",
        "        double t0 = omp_get_wtime();                                     // старт параллельного замера\n",
        "\n",
        "        double sum_par = 0.0;                                            // параллельная сумма\n",
        "\n",
        "        #pragma omp parallel for reduction(+:sum_par) schedule(static)   // параллельный цикл + редукция суммы\n",
        "        for (long long i = 0; i < N; ++i)                                // по всем элементам\n",
        "            sum_par += a[(size_t)i];                                     // суммируем\n",
        "\n",
        "        double mean_par = sum_par / (double)N;                           // среднее\n",
        "\n",
        "        double var_par_acc = 0.0;                                        // накопитель дисперсии\n",
        "\n",
        "        #pragma omp parallel for reduction(+:var_par_acc) schedule(static) // параллельный цикл для дисперсии\n",
        "        for (long long i = 0; i < N; ++i)                                // по всем элементам\n",
        "        {\n",
        "            double d = a[(size_t)i] - mean_par;                          // отклонение\n",
        "            var_par_acc += d * d;                                        // квадрат отклонения\n",
        "        }\n",
        "\n",
        "        double var_par = var_par_acc / (double)N;                        // дисперсия\n",
        "\n",
        "        double t1 = omp_get_wtime();                                     // конец параллельного замера\n",
        "        double time_par = t1 - t0;                                       // время параллельной версии\n",
        "\n",
        "        // Проверим корректность (сравним с последовательной, допуск небольшой)\n",
        "        double eps_mean = std::fabs(mean_par - mean_seq);                // ошибка по mean\n",
        "        double eps_var  = std::fabs(var_par  - var_seq);                 // ошибка по var\n",
        "\n",
        "        // Ускорение и эффективность\n",
        "        double speedup = time_seq / time_par;                            // S = T1 / Tp\n",
        "        double efficiency = speedup / (double)threads;                   // E = S / p\n",
        "\n",
        "        // Оценка доли параллельной части по закону Амдала:\n",
        "        // S(p) = 1 / ( (1-f) + f/p )\n",
        "        // => f = (1 - 1/S) / (1 - 1/p)\n",
        "        double f = 0.0;                                                  // доля параллельной части\n",
        "        if (threads > 1)                                                 // если p>1\n",
        "        {\n",
        "            double invS = 1.0 / speedup;                                 // 1/S\n",
        "            f = (1.0 - invS) / (1.0 - 1.0 / (double)threads);            // формула для f\n",
        "            if (f < 0.0) f = 0.0;                                        // защита\n",
        "            if (f > 1.0) f = 1.0;                                        // защита\n",
        "        }\n",
        "        else\n",
        "        {\n",
        "            f = 0.0;                                                     // при 1 потоке Амдал не оцениваем\n",
        "        }\n",
        "\n",
        "        double serial_part = 1.0 - f;                                    // последовательная доля\n",
        "\n",
        "        std::cout << threads << \",\"                                      // threads\n",
        "                  << time_par << \",\"                                     // time_parallel\n",
        "                  << speedup << \",\"                                      // speedup\n",
        "                  << efficiency << \",\"                                   // efficiency\n",
        "                  << f << \",\"                                            // parallel fraction\n",
        "                  << serial_part << \"\\n\";                                // serial fraction\n",
        "\n",
        "        // Можно ещё вывести ошибки, если нужно (но чтобы не засорять таблицу — закомментировано)\n",
        "        // if (rank == 0) std::cout << \"eps_mean=\" << eps_mean << \" eps_var=\" << eps_var << \"\\n\";\n",
        "        (void)eps_mean;                                                  // чтобы компилятор не ругался, если не выводим\n",
        "        (void)eps_var;                                                   // то же самое\n",
        "    }\n",
        "\n",
        "    return 0;                                                            // конец\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -O2 -fopenmp omp_task1.cpp -o omp_task1\n",
        "!./omp_task1 50000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FmpxY9LTMJM",
        "outputId": "0cf528cc-683e-471e-f8a3-a5e875eb4d63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 50000000\n",
            "Max threads available = 2\n",
            "\n",
            "[SEQ] sum=2.49983e+07 mean=0.499966 var=0.0833294 time=0.138123 s\n",
            "\n",
            "threads,time_parallel,speedup,efficiency,parallel_fraction(f),serial_fraction(1-f)\n",
            "1,0.137311,1.00591,1.00591,0,1\n",
            "2,0.0752641,1.83518,0.917589,0.910187,0.0898129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 2.** Оптимизация доступа к памяти на GPU (CUDA)\n",
        "Реализуйте ядро CUDA для обработки массива данных, демонстрирующее разные\n",
        "паттерны доступа к памяти.\n",
        "\n",
        "**Требуется:**\n",
        "1. реализовать две версии ядра:\n",
        "a. с эффективным (коалесцированным) доступом к глобальной памяти;\n",
        "b. с неэффективным доступом к памяти;\n",
        "2. измерить время выполнения с использованием cudaEvent;\n",
        "3. провести оптимизацию за счёт:\n",
        "a. использования разделяемой памяти;\n",
        "b. изменения организации потоков;\n",
        "4. сравнить результаты и сделать выводы о влиянии доступа к памяти на\n",
        "производительность GPU."
      ],
      "metadata": {
        "id": "DuCZg2VERRoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task2.cu\n",
        "// cuda_mem_patterns.cu — Практическая работа №10, Задание 2\n",
        "// Оптимизация доступа к памяти на GPU (CUDA):\n",
        "// 1) коалесцированный доступ к global memory\n",
        "// 2) некоалесцированный (плохой) доступ по stride\n",
        "// 3) оптимизация с использованием shared memory (tile)\n",
        "// 4) влияние организации потоков (blockSize)\n",
        "// Замер времени через cudaEvent\n",
        "\n",
        "#include <cuda_runtime.h>    // CUDA runtime\n",
        "#include <iostream>          // cout\n",
        "#include <vector>            // vector\n",
        "#include <cmath>             // fabs\n",
        "#include <cstdlib>           // atoi, exit\n",
        "#include <algorithm>         // max\n",
        "\n",
        "// Макрос проверки ошибок CUDA\n",
        "#define CUDA_CHECK(call) do {                                            \\\n",
        "    cudaError_t err = (call);                                            \\\n",
        "    if (err != cudaSuccess) {                                            \\\n",
        "        std::cerr << \"CUDA error: \" << cudaGetErrorString(err)           \\\n",
        "                  << \" at \" << __FILE__ << \":\" << __LINE__ << \"\\n\";      \\\n",
        "        std::exit(1);                                                    \\\n",
        "    }                                                                    \\\n",
        "} while(0)\n",
        "\n",
        "// Ядро 1: коалесцированный доступ (соседние потоки -> соседние элементы)\n",
        "__global__ void kernel_coalesced(const float* __restrict__ in,\n",
        "                                 float* __restrict__ out,\n",
        "                                 int n)\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;   // глобальный индекс\n",
        "    if (i < n)                                       // проверка границ\n",
        "    {\n",
        "        float x = in[i];                             // коалесцированное чтение\n",
        "        out[i] = x * 2.0f + 1.0f;                    // простая операция\n",
        "    }\n",
        "}\n",
        "\n",
        "// Ядро 2: некоалесцированный доступ (stride), соседние потоки читают далеко\n",
        "__global__ void kernel_uncoalesced_stride(const float* __restrict__ in,\n",
        "                                          float* __restrict__ out,\n",
        "                                          int n,\n",
        "                                          int stride)\n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x; // глобальный tid\n",
        "    int i = tid * stride;                            // индекс с шагом stride\n",
        "    if (i < n)                                       // проверка границ\n",
        "    {\n",
        "        float x = in[i];                             // плохой паттерн чтения\n",
        "        out[i] = x * 2.0f + 1.0f;                    // запись разреженная\n",
        "    }\n",
        "}\n",
        "\n",
        "// Ядро 3: shared memory tile\n",
        "// Сначала коалесцированно грузим блок в shared, потом считаем из shared\n",
        "__global__ void kernel_shared_tiled(const float* __restrict__ in,\n",
        "                                    float* __restrict__ out,\n",
        "                                    int n)\n",
        "{\n",
        "    extern __shared__ float tile[];                  // динамическая shared память\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;   // глобальный индекс\n",
        "    int t = threadIdx.x;                             // локальный индекс\n",
        "\n",
        "    if (i < n) tile[t] = in[i];                      // коалесцированная загрузка в shared\n",
        "    else       tile[t] = 0.0f;                       // если вышли за границы\n",
        "\n",
        "    __syncthreads();                                 // ждём загрузки всеми потоками\n",
        "\n",
        "    float x = tile[t];                               // берём из shared\n",
        "    float y = x * 2.0f + 1.0f;                       // считаем\n",
        "\n",
        "    if (i < n) out[i] = y;                           // коалесцированная запись\n",
        "}\n",
        "\n",
        "// Эталон на CPU\n",
        "static void cpu_ref(const std::vector<float>& in, std::vector<float>& out)\n",
        "{\n",
        "    for (size_t i = 0; i < in.size(); ++i)           // по всем элементам\n",
        "        out[i] = in[i] * 2.0f + 1.0f;                // та же формула\n",
        "}\n",
        "\n",
        "// Замер времени для ЛЮБОГО launch-кода (лямбды) через cudaEvent\n",
        "// Важно: здесь НЕТ <<< >>>, мы просто вызываем launch() внутри цикла\n",
        "template <typename LaunchFn>\n",
        "float time_launch(LaunchFn launch, int iters)\n",
        "{\n",
        "    cudaEvent_t start, stop;                         // события CUDA\n",
        "    CUDA_CHECK(cudaEventCreate(&start));             // создаём start\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));              // создаём stop\n",
        "\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());             // синхронизируем перед замером\n",
        "    CUDA_CHECK(cudaEventRecord(start));              // ставим старт\n",
        "\n",
        "    for (int i = 0; i < iters; ++i)                  // повторяем много раз\n",
        "        launch();                                     // запускаем то, что передали\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(stop));               // ставим стоп\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));          // ждём стоп\n",
        "    CUDA_CHECK(cudaGetLastError());                  // ловим ошибки ядра (если есть)\n",
        "\n",
        "    float ms = 0.0f;                                 // миллисекунды\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop)); // считаем время\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(start));             // удаляем события\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "    return ms;                                       // время за iters запусков\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    int N = 1 << 24;                                 // по умолчанию ~16 млн\n",
        "    if (argc > 1) N = std::atoi(argv[1]);            // N из аргументов\n",
        "    if (N <= 0) { std::cout << \"N должно быть > 0\\n\"; return 0; }\n",
        "\n",
        "    int iters = 50;                                  // число повторов для стабильного времени\n",
        "    int stride = 32;                                 // stride для плохого доступа\n",
        "\n",
        "    // Данные на CPU\n",
        "    std::vector<float> h_in(N);                      // вход\n",
        "    std::vector<float> h_out(N, 0.0f);               // выход\n",
        "    std::vector<float> h_ref(N, 0.0f);               // эталон\n",
        "\n",
        "    for (int i = 0; i < N; ++i)                      // заполняем вход\n",
        "        h_in[i] = (float)(i % 100) * 0.01f;          // простые повторяющиеся числа\n",
        "\n",
        "    cpu_ref(h_in, h_ref);                            // эталон\n",
        "\n",
        "    // Данные на GPU\n",
        "    float* d_in = nullptr;                           // вход на GPU\n",
        "    float* d_out = nullptr;                          // выход на GPU\n",
        "    CUDA_CHECK(cudaMalloc(&d_in,  N * sizeof(float)));// malloc input\n",
        "    CUDA_CHECK(cudaMalloc(&d_out, N * sizeof(float)));// malloc output\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(d_in, h_in.data(),\n",
        "                          N * sizeof(float),\n",
        "                          cudaMemcpyHostToDevice));  // копируем на GPU\n",
        "\n",
        "    // Проверяем разные blockSize (организация потоков)\n",
        "    std::vector<int> block_sizes = {128, 256, 512};   // варианты\n",
        "\n",
        "    std::cout << \"N=\" << N << \", iters=\" << iters << \", stride=\" << stride << \"\\n\";\n",
        "    std::cout << \"Columns: block, coalesced_ms, uncoalesced_ms, shared_ms\\n\";\n",
        "\n",
        "    for (int bs : block_sizes)                        // перебор размеров блока\n",
        "    {\n",
        "        dim3 block(bs);                               // блок\n",
        "        dim3 grid((N + bs - 1) / bs);                 // сетка для обычных ядер\n",
        "\n",
        "        // 1) коалесцированное ядро\n",
        "        auto launch_coal = [&]() {                    // лямбда запуска\n",
        "            kernel_coalesced<<<grid, block>>>(d_in, d_out, N);\n",
        "        };\n",
        "        float ms_coal = time_launch(launch_coal, iters); // время за iters\n",
        "\n",
        "        // 2) плохой доступ: ядро обрабатывает индексы i=tid*stride\n",
        "        int n_bad = (N + stride - 1) / stride;        // сколько \"полезных\" tid\n",
        "        dim3 grid_bad((n_bad + bs - 1) / bs);         // сетка для плохого ядра\n",
        "\n",
        "        auto launch_uncoal = [&]() {                  // лямбда запуска\n",
        "            kernel_uncoalesced_stride<<<grid_bad, block>>>(d_in, d_out, N, stride);\n",
        "        };\n",
        "        float ms_uncoal = time_launch(launch_uncoal, iters); // время\n",
        "\n",
        "        // 3) shared memory ядро\n",
        "        size_t shmem = (size_t)bs * sizeof(float);    // shared на блок\n",
        "        auto launch_shared = [&]() {                  // лямбда запуска\n",
        "            kernel_shared_tiled<<<grid, block, shmem>>>(d_in, d_out, N);\n",
        "        };\n",
        "        float ms_shared = time_launch(launch_shared, iters); // время\n",
        "\n",
        "        // Печать среднего времени одного запуска (делим на iters)\n",
        "        std::cout << bs << \", \"\n",
        "                  << (ms_coal / iters) << \", \"\n",
        "                  << (ms_uncoal / iters) << \", \"\n",
        "                  << (ms_shared / iters) << \"\\n\";\n",
        "    }\n",
        "\n",
        "    // Проверка корректности (на коалесцированном ядре)\n",
        "    {\n",
        "        int bs = 256;                                 // block size для проверки\n",
        "        dim3 block(bs);                               // block\n",
        "        dim3 grid((N + bs - 1) / bs);                 // grid\n",
        "        kernel_coalesced<<<grid, block>>>(d_in, d_out, N); // запуск\n",
        "        CUDA_CHECK(cudaDeviceSynchronize());          // ждём\n",
        "        CUDA_CHECK(cudaMemcpy(h_out.data(), d_out,\n",
        "                              N * sizeof(float),\n",
        "                              cudaMemcpyDeviceToHost)); // копируем назад\n",
        "\n",
        "        double max_err = 0.0;                         // максимум ошибки\n",
        "        for (int i = 0; i < N; ++i)                   // сравнение\n",
        "        {\n",
        "            double e = std::fabs((double)h_out[i] - (double)h_ref[i]); // ошибка\n",
        "            if (e > max_err) max_err = e;             // обновили максимум\n",
        "        }\n",
        "        std::cout << \"Max abs error (coalesced vs CPU ref) = \" << max_err << \"\\n\";\n",
        "    }\n",
        "\n",
        "    CUDA_CHECK(cudaFree(d_in));                       // освобождаем input\n",
        "    CUDA_CHECK(cudaFree(d_out));                      // освобождаем output\n",
        "    return 0;                                         // конец\n",
        "}\n"
      ],
      "metadata": {
        "id": "IbvdIky_RR2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad702d27-4081-42eb-c5df-ec93c9ecd906"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O2 -gencode arch=compute_75,code=sm_75 task2.cu -o task2\n",
        "!./task2 8000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5K8vAVhXK4i",
        "outputId": "c731d12b-400a-45da-daaa-1ce531b6aab6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N=8000000, iters=50, stride=32\n",
            "Columns: block, coalesced_ms, uncoalesced_ms, shared_ms\n",
            "128, 0.272735, 0.226547, 0.300196\n",
            "256, 0.271114, 0.230157, 0.30593\n",
            "512, 0.284988, 0.23511, 0.327761\n",
            "Max abs error (coalesced vs CPU ref) = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./cuda_mem_patterns 10000000"
      ],
      "metadata": {
        "id": "W5Xc0zXHXQsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 3.** Профилирование гибридного приложения CPU + GPU\n",
        "Разработайте гибридную программу, в которой часть вычислений выполняется на CPU, а часть — на GPU.\n",
        "\n",
        "**Требуется:**\n",
        "1. реализовать гибридный алгоритм обработки массива данных;\n",
        "2. использовать асинхронную передачу данных (cudaMemcpyAsync) и CUDA streams;\n",
        "3. выполнить профилирование приложения:\n",
        "a. определить накладные расходы передачи данных;\n",
        "b. выявить узкие места при взаимодействии CPU и GPU;\n",
        "4. предложить и реализовать одну оптимизацию, уменьшающую накладные расходы."
      ],
      "metadata": {
        "id": "PUPUG2oxRR_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task3.cu\n",
        "// 1) Гибридная обработка массива: CPU считает часть, GPU считает часть (out[i] = in[i]*2 + 1)\n",
        "// 2) Асинхронные копии cudaMemcpyAsync + CUDA streams + пайплайн по чанкам\n",
        "// 3) Профилирование: отдельно считаем H2D / kernel / D2H / total и показываем узкие места\n",
        "// 4) Оптимизация: pinned memory + overlap (две stream-ленты, двойная буферизация)\n",
        "\n",
        "#include <cuda_runtime.h>     // CUDA runtime\n",
        "#include <iostream>          // cout\n",
        "#include <vector>            // vector\n",
        "#include <cmath>             // fabs\n",
        "#include <cstdlib>           // atoi, atoll\n",
        "#include <chrono>            // chrono для CPU-тайминга\n",
        "#include <algorithm>         // max\n",
        "#include <cstring>\n",
        "\n",
        "// Макрос проверки ошибок CUDA\n",
        "#define CUDA_CHECK(call) do {                                                \\\n",
        "    cudaError_t err = (call);                                                \\\n",
        "    if (err != cudaSuccess) {                                                \\\n",
        "        std::cerr << \"CUDA error: \" << cudaGetErrorString(err)               \\\n",
        "                  << \" at \" << __FILE__ << \":\" << __LINE__ << \"\\n\";          \\\n",
        "        std::exit(1);                                                        \\\n",
        "    }                                                                        \\\n",
        "} while(0)\n",
        "\n",
        "// Простое GPU-ядро: out[i] = in[i] * 2 + 1\n",
        "__global__ void kernel_affine(const float* __restrict__ in,\n",
        "                              float* __restrict__ out,\n",
        "                              int n)\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;   // глобальный индекс\n",
        "    if (i < n)                                       // проверка границ\n",
        "        out[i] = in[i] * 2.0f + 1.0f;                // вычисление\n",
        "}\n",
        "\n",
        "// CPU-референс (для проверки корректности)\n",
        "static void cpu_ref(const float* in, float* out, int n)\n",
        "{\n",
        "    for (int i = 0; i < n; ++i)                      // по элементам\n",
        "        out[i] = in[i] * 2.0f + 1.0f;                // то же самое\n",
        "}\n",
        "\n",
        "// CPU-часть гибридного алгоритма: считаем свою долю массива на CPU\n",
        "static void cpu_process_part(const float* in, float* out, int begin, int end)\n",
        "{\n",
        "    for (int i = begin; i < end; ++i)                // диапазон CPU\n",
        "        out[i] = in[i] * 2.0f + 1.0f;                // вычисление на CPU\n",
        "}\n",
        "\n",
        "// Вспомогательная печать времени (ms)\n",
        "static double ms_between(std::chrono::high_resolution_clock::time_point a,\n",
        "                         std::chrono::high_resolution_clock::time_point b)\n",
        "{\n",
        "    return std::chrono::duration<double, std::milli>(b - a).count();        // разница в мс\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    // параметры\n",
        "    long long Nll = 16'000'000;                                            // размер массива по умолчанию\n",
        "    if (argc > 1) Nll = std::atoll(argv[1]);                               // N из аргументов\n",
        "    if (Nll <= 0) { std::cout << \"N должно быть > 0\\n\"; return 0; }        // проверка\n",
        "    if (Nll > (long long)2e9) { std::cout << \"Слишком большой N\\n\"; return 0; } // защита\n",
        "\n",
        "    int N = (int)Nll;                                                      // перевод в int для CUDA\n",
        "    int cpu_share_percent = 25;                                            // сколько процентов обрабатывает CPU\n",
        "    int cpu_n = (N * cpu_share_percent) / 100;                             // размер CPU-части\n",
        "    int gpu_n = N - cpu_n;                                                 // размер GPU-части\n",
        "\n",
        "    int block = 256;                                                       // размер блока CUDA\n",
        "    int chunk = 1 << 20;                                                   // размер чанка для пайплайна (1,048,576 элементов)\n",
        "    if (argc > 2) chunk = std::atoi(argv[2]);                              // chunk из аргументов\n",
        "    if (chunk <= 0) { std::cout << \"chunk должно быть > 0\\n\"; return 0; }  // проверка\n",
        "\n",
        "    std::cout << \"N=\" << N << \" elements\\n\";                               // печать N\n",
        "    std::cout << \"CPU share=\" << cpu_share_percent << \"% (\" << cpu_n << \")\\n\";\n",
        "    std::cout << \"GPU share=\" << (100 - cpu_share_percent) << \"% (\" << gpu_n << \")\\n\";\n",
        "    std::cout << \"chunk=\" << chunk << \" elements\\n\\n\";\n",
        "\n",
        "    //  входные данные (обычная host память)\n",
        "    std::vector<float> h_in(N);                                            // вход на host\n",
        "    std::vector<float> h_out_cpu(N, 0.0f);                                 // выход гибридный\n",
        "    std::vector<float> h_out_ref(N, 0.0f);                                 // эталон\n",
        "\n",
        "    for (int i = 0; i < N; ++i)                                            // заполняем вход\n",
        "        h_in[i] = (float)(i % 100) * 0.01f;                                // повторяемые числа\n",
        "\n",
        "    cpu_ref(h_in.data(), h_out_ref.data(), N);                             // эталон на CPU\n",
        "\n",
        "    // ВАРИАНТ A: \"база\" — синхронные копии (cudaMemcpy) без overlap\n",
        "    // Цель: получить накладные расходы передачи данных как baseline\n",
        "    {\n",
        "        std::cout << \"=== Variant A (baseline): sync copies, no streams overlap ===\\n\";\n",
        "\n",
        "        float* d_in = nullptr;                                             // device input\n",
        "        float* d_out = nullptr;                                            // device output\n",
        "        CUDA_CHECK(cudaMalloc(&d_in,  gpu_n * sizeof(float)));             // malloc GPU-часть\n",
        "        CUDA_CHECK(cudaMalloc(&d_out, gpu_n * sizeof(float)));             // malloc GPU-часть\n",
        "\n",
        "        // CPU таймер (полная длительность)\n",
        "        auto tA0 = std::chrono::high_resolution_clock::now();              // старт total на CPU\n",
        "\n",
        "        // 1) CPU считает свою часть (0..cpu_n)\n",
        "        auto t_cpu0 = std::chrono::high_resolution_clock::now();           // старт CPU части\n",
        "        cpu_process_part(h_in.data(), h_out_cpu.data(), 0, cpu_n);         // CPU вычисления\n",
        "        auto t_cpu1 = std::chrono::high_resolution_clock::now();           // конец CPU части\n",
        "\n",
        "        // 2) GPU считает оставшуюся часть (cpu_n..N) — копии синхронные\n",
        "        // замерим H2D / kernel / D2H через cudaEvent\n",
        "        cudaEvent_t e0, e1, e2, e3;                                        // события\n",
        "        CUDA_CHECK(cudaEventCreate(&e0));                                  // create\n",
        "        CUDA_CHECK(cudaEventCreate(&e1));\n",
        "        CUDA_CHECK(cudaEventCreate(&e2));\n",
        "        CUDA_CHECK(cudaEventCreate(&e3));\n",
        "\n",
        "        CUDA_CHECK(cudaEventRecord(e0));                                   // отметка перед H2D\n",
        "        CUDA_CHECK(cudaMemcpy(d_in, h_in.data() + cpu_n,                   // H2D (синхронно)\n",
        "                              gpu_n * sizeof(float),\n",
        "                              cudaMemcpyHostToDevice));\n",
        "        CUDA_CHECK(cudaEventRecord(e1));                                   // отметка после H2D\n",
        "\n",
        "        int grid = (gpu_n + block - 1) / block;                            // grid\n",
        "        kernel_affine<<<grid, block>>>(d_in, d_out, gpu_n);                // kernel\n",
        "        CUDA_CHECK(cudaGetLastError());                                    // проверка\n",
        "        CUDA_CHECK(cudaEventRecord(e2));                                   // отметка после kernel\n",
        "\n",
        "        CUDA_CHECK(cudaMemcpy(h_out_cpu.data() + cpu_n, d_out,            // D2H (синхронно)\n",
        "                              gpu_n * sizeof(float),\n",
        "                              cudaMemcpyDeviceToHost));\n",
        "        CUDA_CHECK(cudaEventRecord(e3));                                   // отметка после D2H\n",
        "\n",
        "        CUDA_CHECK(cudaEventSynchronize(e3));                              // ждём окончания всего\n",
        "        auto tA1 = std::chrono::high_resolution_clock::now();              // конец total на CPU\n",
        "\n",
        "        float ms_h2d=0, ms_k=0, ms_d2h=0;                                  // времена\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&ms_h2d, e0, e1));                 // H2D\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&ms_k,   e1, e2));                 // kernel (примерно)\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&ms_d2h, e2, e3));                 // D2H\n",
        "\n",
        "        double ms_cpu = ms_between(t_cpu0, t_cpu1);                        // время CPU части\n",
        "        double ms_total = ms_between(tA0, tA1);                            // полное время\n",
        "\n",
        "        // Проверка корректности (макс. ошибка)\n",
        "        double max_err = 0.0;                                              // максимум ошибки\n",
        "        for (int i = 0; i < N; ++i)                                        // сравнение\n",
        "            max_err = std::max(max_err, (double)std::fabs(h_out_cpu[i] - h_out_ref[i]));\n",
        "\n",
        "        std::cout << \"CPU part time (ms): \" << ms_cpu << \"\\n\";             // CPU время\n",
        "        std::cout << \"GPU H2D time (ms): \" << ms_h2d << \"\\n\";              // H2D\n",
        "        std::cout << \"GPU kernel time (ms): \" << ms_k << \"\\n\";             // kernel\n",
        "        std::cout << \"GPU D2H time (ms): \" << ms_d2h << \"\\n\";              // D2H\n",
        "        std::cout << \"TOTAL time (ms): \" << ms_total << \"\\n\";              // total\n",
        "        std::cout << \"Max abs error: \" << max_err << \"\\n\\n\";               // корректность\n",
        "\n",
        "        CUDA_CHECK(cudaEventDestroy(e0));                                  // destroy events\n",
        "        CUDA_CHECK(cudaEventDestroy(e1));\n",
        "        CUDA_CHECK(cudaEventDestroy(e2));\n",
        "        CUDA_CHECK(cudaEventDestroy(e3));\n",
        "\n",
        "        CUDA_CHECK(cudaFree(d_in));                                        // free\n",
        "        CUDA_CHECK(cudaFree(d_out));\n",
        "    }\n",
        "\n",
        "    // ВАРИАНТ B: оптимизированный — pinned memory + cudaMemcpyAsync + streams\n",
        "    // Идея: делим GPU-часть на чанки и делаем пайплайн (двойная буферизация)\n",
        "    // Оптимизация (п.4): pinned memory + overlap H2D/kernel/D2H\n",
        "    {\n",
        "        std::cout << \"=== Variant B (optimized): pinned + cudaMemcpyAsync + 2 streams (overlap) ===\\n\";\n",
        "\n",
        "        // 1) Выделяем pinned память под GPU-часть входа/выхода (уменьшаем overhead копий)\n",
        "        float* h_in_pinned = nullptr;                                      // pinned input для GPU-части\n",
        "        float* h_out_pinned = nullptr;                                     // pinned output для GPU-части\n",
        "        CUDA_CHECK(cudaMallocHost(&h_in_pinned,  gpu_n * sizeof(float)));  // pinned host input\n",
        "        CUDA_CHECK(cudaMallocHost(&h_out_pinned, gpu_n * sizeof(float)));  // pinned host output\n",
        "\n",
        "        // копируем данные GPU-части из vector в pinned (один раз)\n",
        "        std::memcpy(h_in_pinned,  h_in.data()  + cpu_n, gpu_n * sizeof(float));\n",
        "        std::memset(h_out_pinned, 0, gpu_n * sizeof(float));\n",
        "\n",
        "        // 2) Две stream для двойной буферизации (ping-pong)\n",
        "        cudaStream_t s0, s1;                                               // два потока CUDA\n",
        "        CUDA_CHECK(cudaStreamCreate(&s0));                                 // create stream 0\n",
        "        CUDA_CHECK(cudaStreamCreate(&s1));                                 // create stream 1\n",
        "\n",
        "        // 3) Два device-буфера под чанки (ping-pong)\n",
        "        float* d_in0=nullptr; float* d_out0=nullptr;                       // буферы для stream0\n",
        "        float* d_in1=nullptr; float* d_out1=nullptr;                       // буферы для stream1\n",
        "        CUDA_CHECK(cudaMalloc(&d_in0,  chunk * sizeof(float)));            // alloc chunk\n",
        "        CUDA_CHECK(cudaMalloc(&d_out0, chunk * sizeof(float)));\n",
        "        CUDA_CHECK(cudaMalloc(&d_in1,  chunk * sizeof(float)));\n",
        "        CUDA_CHECK(cudaMalloc(&d_out1, chunk * sizeof(float)));\n",
        "\n",
        "        // 4) События для профилирования (накладные передачи)\n",
        "        cudaEvent_t e_start, e_end;                                        // события total GPU pipeline\n",
        "        CUDA_CHECK(cudaEventCreate(&e_start));\n",
        "        CUDA_CHECK(cudaEventCreate(&e_end));\n",
        "\n",
        "        // CPU таймер полного времени\n",
        "        auto tB0 = std::chrono::high_resolution_clock::now();              // старт total на CPU\n",
        "\n",
        "        // CPU считает свою часть параллельно с GPU-пайплайном (на практике это и есть гибрид)\n",
        "        auto t_cpu0 = std::chrono::high_resolution_clock::now();           // старт CPU части\n",
        "        cpu_process_part(h_in.data(), h_out_cpu.data(), 0, cpu_n);         // CPU вычисления\n",
        "        auto t_cpu1 = std::chrono::high_resolution_clock::now();           // конец CPU части\n",
        "\n",
        "        CUDA_CHECK(cudaEventRecord(e_start, 0));                           // старт GPU-пайплайна (на default stream)\n",
        "\n",
        "        // Для профилирования отдельно посчитаем суммарные времена H2D/D2H/Kernel по событиям на чанках\n",
        "        double sum_h2d_ms = 0.0;                                           // суммарное время H2D\n",
        "        double sum_d2h_ms = 0.0;                                           // суммарное время D2H\n",
        "        double sum_k_ms   = 0.0;                                           // суммарное время kernel\n",
        "\n",
        "        // Число чанков\n",
        "        int chunks = (gpu_n + chunk - 1) / chunk;                          // сколько чанков\n",
        "\n",
        "        // Для каждого чанка: async H2D -> kernel -> async D2H в своей stream\n",
        "        for (int c = 0; c < chunks; ++c)                                   // по чанкам\n",
        "        {\n",
        "            int offset = c * chunk;                                        // смещение в GPU-части\n",
        "            int cur = std::min(chunk, gpu_n - offset);                     // текущий размер чанка\n",
        "\n",
        "            cudaStream_t stream = (c % 2 == 0) ? s0 : s1;                  // выбираем stream\n",
        "            float* d_in  = (c % 2 == 0) ? d_in0  : d_in1;                  // выбираем device input\n",
        "            float* d_out = (c % 2 == 0) ? d_out0 : d_out1;                 // выбираем device output\n",
        "\n",
        "            // События на конкретный чанк, чтобы оценить стадии (создаём/удаляем быстро, но можно и пулом)\n",
        "            cudaEvent_t eh0, eh1, ek0, ek1, ed0, ed1;                      // события стадий\n",
        "            CUDA_CHECK(cudaEventCreate(&eh0)); CUDA_CHECK(cudaEventCreate(&eh1));\n",
        "            CUDA_CHECK(cudaEventCreate(&ek0)); CUDA_CHECK(cudaEventCreate(&ek1));\n",
        "            CUDA_CHECK(cudaEventCreate(&ed0)); CUDA_CHECK(cudaEventCreate(&ed1));\n",
        "\n",
        "            CUDA_CHECK(cudaEventRecord(eh0, stream));                      // отметка H2D start\n",
        "            CUDA_CHECK(cudaMemcpyAsync(d_in, h_in_pinned + offset,         // H2D async\n",
        "                                       cur * sizeof(float),\n",
        "                                       cudaMemcpyHostToDevice, stream));\n",
        "            CUDA_CHECK(cudaEventRecord(eh1, stream));                      // отметка H2D end\n",
        "\n",
        "            int grid = (cur + block - 1) / block;                          // grid для чанка\n",
        "\n",
        "            CUDA_CHECK(cudaEventRecord(ek0, stream));                      // kernel start\n",
        "            kernel_affine<<<grid, block, 0, stream>>>(d_in, d_out, cur);   // kernel в stream\n",
        "            CUDA_CHECK(cudaGetLastError());                                // проверка\n",
        "            CUDA_CHECK(cudaEventRecord(ek1, stream));                      // kernel end\n",
        "\n",
        "            CUDA_CHECK(cudaEventRecord(ed0, stream));                      // D2H start\n",
        "            CUDA_CHECK(cudaMemcpyAsync(h_out_pinned + offset, d_out,       // D2H async\n",
        "                                       cur * sizeof(float),\n",
        "                                       cudaMemcpyDeviceToHost, stream));\n",
        "            CUDA_CHECK(cudaEventRecord(ed1, stream));                      // D2H end\n",
        "\n",
        "            // Ждём завершение этого чанка, чтобы снять времена стадий (для профилирования)\n",
        "            CUDA_CHECK(cudaEventSynchronize(ed1));                         // ждём конец D2H по чанку\n",
        "\n",
        "            float h2d=0, k=0, d2h=0;                                       // локальные времена\n",
        "            CUDA_CHECK(cudaEventElapsedTime(&h2d, eh0, eh1));              // H2D время чанка\n",
        "            CUDA_CHECK(cudaEventElapsedTime(&k,   ek0, ek1));              // kernel время чанка\n",
        "            CUDA_CHECK(cudaEventElapsedTime(&d2h, ed0, ed1));              // D2H время чанка\n",
        "\n",
        "            sum_h2d_ms += h2d;                                             // суммируем\n",
        "            sum_k_ms   += k;\n",
        "            sum_d2h_ms += d2h;\n",
        "\n",
        "            CUDA_CHECK(cudaEventDestroy(eh0)); CUDA_CHECK(cudaEventDestroy(eh1)); // destroy\n",
        "            CUDA_CHECK(cudaEventDestroy(ek0)); CUDA_CHECK(cudaEventDestroy(ek1));\n",
        "            CUDA_CHECK(cudaEventDestroy(ed0)); CUDA_CHECK(cudaEventDestroy(ed1));\n",
        "        }\n",
        "\n",
        "        // Дождёмся всех stream (на случай если что-то осталось)\n",
        "        CUDA_CHECK(cudaStreamSynchronize(s0));                             // sync stream0\n",
        "        CUDA_CHECK(cudaStreamSynchronize(s1));                             // sync stream1\n",
        "\n",
        "        CUDA_CHECK(cudaEventRecord(e_end, 0));                             // конец пайплайна\n",
        "        CUDA_CHECK(cudaEventSynchronize(e_end));                           // ждём\n",
        "\n",
        "        float pipeline_ms = 0.0f;                                          // общее время GPU pipeline (по событию)\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&pipeline_ms, e_start, e_end));    // считаем\n",
        "\n",
        "        // Копируем pinned output обратно в общий h_out_cpu в диапазон GPU-части\n",
        "        std::memcpy(h_out_cpu.data() + cpu_n, h_out_pinned, gpu_n * sizeof(float));\n",
        "\n",
        "        auto tB1 = std::chrono::high_resolution_clock::now();              // конец total на CPU\n",
        "\n",
        "        double ms_cpu = ms_between(t_cpu0, t_cpu1);                        // CPU часть (внутри total)\n",
        "        double ms_total = ms_between(tB0, tB1);                            // полное время программы\n",
        "\n",
        "        // Проверка корректности (макс. ошибка)\n",
        "        double max_err = 0.0;                                              // максимум ошибки\n",
        "        for (int i = 0; i < N; ++i)                                        // сравнение\n",
        "            max_err = std::max(max_err, (double)std::fabs(h_out_cpu[i] - h_out_ref[i]));\n",
        "\n",
        "        std::cout << \"CPU part time (ms): \" << ms_cpu << \"\\n\";             // CPU время\n",
        "        std::cout << \"GPU pipeline total (ms, overlapped): \" << pipeline_ms << \"\\n\";\n",
        "        std::cout << \"Sum H2D over chunks (ms): \" << sum_h2d_ms << \"\\n\";   // суммарно, без учёта overlap\n",
        "        std::cout << \"Sum kernel over chunks (ms): \" << sum_k_ms << \"\\n\";\n",
        "        std::cout << \"Sum D2H over chunks (ms): \" << sum_d2h_ms << \"\\n\";\n",
        "        std::cout << \"TOTAL time (ms): \" << ms_total << \"\\n\";              // total\n",
        "        std::cout << \"Max abs error: \" << max_err << \"\\n\\n\";               // корректность\n",
        "\n",
        "        // Убираем ресурсы\n",
        "        CUDA_CHECK(cudaEventDestroy(e_start));                              // destroy events\n",
        "        CUDA_CHECK(cudaEventDestroy(e_end));\n",
        "\n",
        "        CUDA_CHECK(cudaFree(d_in0)); CUDA_CHECK(cudaFree(d_out0));          // free device buffers\n",
        "        CUDA_CHECK(cudaFree(d_in1)); CUDA_CHECK(cudaFree(d_out1));\n",
        "\n",
        "        CUDA_CHECK(cudaStreamDestroy(s0));                                  // destroy streams\n",
        "        CUDA_CHECK(cudaStreamDestroy(s1));\n",
        "\n",
        "        CUDA_CHECK(cudaFreeHost(h_in_pinned));                              // free pinned\n",
        "        CUDA_CHECK(cudaFreeHost(h_out_pinned));\n",
        "    }\n",
        "\n",
        "    return 0;                                                               // конец\n",
        "}"
      ],
      "metadata": {
        "id": "VOwO5v_aRSN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8ad7fc-1b46-4ab7-ce8b-ca90650f453a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O2 -arch=sm_75 task3.cu -o task3\n",
        "!./task3 8000000 1048576"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th76fs-WaLbC",
        "outputId": "bebfff44-3c9c-4cc1-a3ca-861be296cb8f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N=8000000 elements\n",
            "CPU share=25% (2000000)\n",
            "GPU share=75% (6000000)\n",
            "chunk=1048576 elements\n",
            "\n",
            "=== Variant A (baseline): sync copies, no streams overlap ===\n",
            "CPU part time (ms): 1.54413\n",
            "GPU H2D time (ms): 5.30106\n",
            "GPU kernel time (ms): 0.244896\n",
            "GPU D2H time (ms): 5.10989\n",
            "TOTAL time (ms): 12.238\n",
            "Max abs error: 0\n",
            "\n",
            "=== Variant B (optimized): pinned + cudaMemcpyAsync + 2 streams (overlap) ===\n",
            "CPU part time (ms): 2.02817\n",
            "GPU pipeline total (ms, overlapped): 4.37411\n",
            "Sum H2D over chunks (ms): 2.04186\n",
            "Sum kernel over chunks (ms): 0.240544\n",
            "Sum D2H over chunks (ms): 1.88282\n",
            "TOTAL time (ms): 10.9521\n",
            "Max abs error: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для оценки накладных расходов передачи данных между CPU и GPU была выполнена детализация времени выполнения на этапы:\n",
        "\n",
        "* вычисления на CPU;\n",
        "\n",
        "* передача данных Host → Device (H2D);\n",
        "\n",
        "* выполнение CUDA-ядра;\n",
        "\n",
        "* передача данных Device → Host (D2H).\n",
        "\n",
        "В базовой реализации (Variant A) использовались синхронные передачи данных (cudaMemcpy) без перекрытия вычислений и копирования.\n",
        "\n",
        "В оптимизированной версии (Variant B) были применены следующие подходы:\n",
        "\n",
        "* использование закреплённой (pinned) памяти на стороне host;\n",
        "\n",
        "* асинхронные передачи данных (cudaMemcpyAsync);\n",
        "\n",
        "* организация вычислений в двух CUDA streams с разбиением данных на чанки.\n",
        "\n",
        "Полученные данные подтверждают, что использование pinned memory и асинхронных передач позволяет существенно сократить накладные расходы обмена данными."
      ],
      "metadata": {
        "id": "xo5k_xlmbSEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Анализ профилирования показал, что в базовой реализации основным узким местом приложения является передача данных между CPU и GPU:\n",
        "\n",
        "* суммарное время передач (H2D + D2H) составляет ≈ 10.41 мс;\n",
        "\n",
        "* время выполнения вычислительного ядра на GPU составляет менее 0.3 мс.\n",
        "\n",
        "Следовательно, приложение является memory-bound по передаче данных, а не вычислительно ограниченным. Производительность ограничивается пропускной способностью шины передачи данных между host и device, а не скоростью выполнения CUDA-ядра."
      ],
      "metadata": {
        "id": "Djn6tq7BbmY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 4.** Анализ масштабируемости распределённой программы (MPI)\n",
        "Реализуйте распределённую программу на MPI для вычисления агрегатной функции над\n",
        "большим массивом (например, сумма, минимум, максимум).\n",
        "\n",
        "**Требуется:**\n",
        "* измерить время выполнения при различном числе процессов;\n",
        "* оценить strong scaling и weak scaling;\n",
        "* проанализировать влияние коммуникационных операций (MPI_Reduce,\n",
        "MPI_Allreduce);\n",
        "* сделать вывод о масштабируемости алгоритма и его практических ограничениях"
      ],
      "metadata": {
        "id": "ipMEbtKQRt6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task4.cpp\n",
        "\n",
        "#include <mpi.h>                 // MPI\n",
        "#include <iostream>              // cout\n",
        "#include <vector>                // vector\n",
        "#include <random>                // mt19937\n",
        "#include <limits>                // numeric_limits\n",
        "#include <cstdlib>               // atoll, atoi\n",
        "#include <algorithm>             // min, max\n",
        "\n",
        "struct AggLocal                                                      // локальные агрегаты\n",
        "{\n",
        "    double sum;                                                      // сумма\n",
        "    double minv;                                                     // минимум\n",
        "    double maxv;                                                     // максимум\n",
        "};\n",
        "\n",
        "// заполнение локального массива случайными числами\n",
        "static void fill_random(std::vector<double>& a, int seed)            // генерация\n",
        "{\n",
        "    std::mt19937 gen(seed);                                          // генератор\n",
        "    std::uniform_real_distribution<double> dist(0.0, 1.0);           // 0..1\n",
        "    for (size_t i = 0; i < a.size(); ++i)                            // цикл\n",
        "        a[i] = dist(gen);                                            // значение\n",
        "}\n",
        "\n",
        "// вычисление локальных sum/min/max\n",
        "static AggLocal local_agg(const std::vector<double>& a)              // локальная агрегация\n",
        "{\n",
        "    AggLocal r;                                                      // результат\n",
        "    r.sum = 0.0;                                                     // старт суммы\n",
        "    r.minv = std::numeric_limits<double>::infinity();                // старт минимума\n",
        "    r.maxv = -std::numeric_limits<double>::infinity();               // старт максимума\n",
        "\n",
        "    for (size_t i = 0; i < a.size(); ++i)                            // по массиву\n",
        "    {\n",
        "        double x = a[i];                                             // элемент\n",
        "        r.sum += x;                                                  // суммируем\n",
        "        if (x < r.minv) r.minv = x;                                  // минимум\n",
        "        if (x > r.maxv) r.maxv = x;                                  // максимум\n",
        "    }\n",
        "    return r;                                                        // вернуть\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv)                                     // вход\n",
        "{\n",
        "    MPI_Init(&argc, &argv);                                         // старт MPI\n",
        "\n",
        "    int rank = 0;                                                   // rank\n",
        "    int size = 0;                                                   // size\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);                           // rank\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);                           // size\n",
        "\n",
        "    // режимы:\n",
        "    // strong: общий N фиксированный\n",
        "    // weak: N_per_proc фиксированный, общий N = N_per_proc * size\n",
        "    std::string mode = \"strong\";                                    // режим по умолчанию\n",
        "    if (argc > 1) mode = argv[1];                                   // mode из argv[1]\n",
        "\n",
        "    long long N_total = 50'000'000;                                 // общий N для strong\n",
        "    long long N_per_proc = 10'000'000;                              // N на процесс для weak\n",
        "    int use_allreduce = 0;                                          // 0 = Reduce, 1 = Allreduce\n",
        "\n",
        "    if (mode == \"strong\")                                           // strong scaling\n",
        "    {\n",
        "        if (argc > 2) N_total = std::atoll(argv[2]);                // N_total\n",
        "        if (argc > 3) use_allreduce = std::atoi(argv[3]);           // reduce/allreduce\n",
        "    }\n",
        "    else if (mode == \"weak\")                                        // weak scaling\n",
        "    {\n",
        "        if (argc > 2) N_per_proc = std::atoll(argv[2]);             // N_per_proc\n",
        "        if (argc > 3) use_allreduce = std::atoi(argv[3]);           // reduce/allreduce\n",
        "        N_total = N_per_proc * (long long)size;                     // общий N растёт с size\n",
        "    }\n",
        "    else                                                             // неверный режим\n",
        "    {\n",
        "        if (rank == 0)\n",
        "        {\n",
        "            std::cout << \"Неверный режим. Используй:\\n\";\n",
        "            std::cout << \"  strong N_total use_allreduce(0/1)\\n\";\n",
        "            std::cout << \"  weak   N_per_proc use_allreduce(0/1)\\n\";\n",
        "        }\n",
        "        MPI_Finalize();                                             // конец MPI\n",
        "        return 0;                                                   // выход\n",
        "    }\n",
        "\n",
        "    // распределение: делим N_total по процессам с остатком\n",
        "    long long base = N_total / size;                                // базовый размер\n",
        "    long long rem  = N_total % size;                                // остаток\n",
        "    long long local_n = base + (rank < rem ? 1 : 0);                // размер локального куска\n",
        "\n",
        "    // создаём локальный массив\n",
        "    std::vector<double> local_data((size_t)local_n);                // локальные данные\n",
        "    fill_random(local_data, 1234 + rank);                           // заполняем (seed разный на rank)\n",
        "\n",
        "    MPI_Barrier(MPI_COMM_WORLD);                                    // синхронизация перед замером\n",
        "\n",
        "    // отдельно меряем: compute_time и comm_time, и total\n",
        "    double t0_total = MPI_Wtime();                                  // старт total\n",
        "\n",
        "    double t0_comp = MPI_Wtime();                                   // старт compute\n",
        "    AggLocal loc = local_agg(local_data);                           // локальная агрегация\n",
        "    double t1_comp = MPI_Wtime();                                   // конец compute\n",
        "\n",
        "    double compute_time = t1_comp - t0_comp;                        // время вычислений\n",
        "\n",
        "    // глобальные агрегаты\n",
        "    double gsum = 0.0;                                              // глобальная сумма\n",
        "    double gmin = 0.0;                                              // глобальный минимум\n",
        "    double gmax = 0.0;                                              // глобальный максимум\n",
        "\n",
        "    double t0_comm = MPI_Wtime();                                   // старт коммуникаций\n",
        "\n",
        "    if (use_allreduce == 0)                                         // вариант с MPI_Reduce\n",
        "    {\n",
        "        MPI_Reduce(&loc.sum,  &gsum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD); // sum\n",
        "        MPI_Reduce(&loc.minv, &gmin, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD); // min\n",
        "        MPI_Reduce(&loc.maxv, &gmax, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD); // max\n",
        "    }\n",
        "    else                                                            // вариант с MPI_Allreduce\n",
        "    {\n",
        "        MPI_Allreduce(&loc.sum,  &gsum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); // sum\n",
        "        MPI_Allreduce(&loc.minv, &gmin, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD); // min\n",
        "        MPI_Allreduce(&loc.maxv, &gmax, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD); // max\n",
        "    }\n",
        "\n",
        "    double t1_comm = MPI_Wtime();                                   // конец коммуникаций\n",
        "    double comm_time = t1_comm - t0_comm;                           // время коммуникаций\n",
        "\n",
        "    double t1_total = MPI_Wtime();                                  // конец total\n",
        "    double total_time = t1_total - t0_total;                        // total время\n",
        "\n",
        "    // берём максимальные времена по процессам (чтобы честно: кто самый медленный, тот и определяет время)\n",
        "    double comp_max = 0.0;                                          // максимум compute\n",
        "    double comm_max = 0.0;                                          // максимум comm\n",
        "    double total_max = 0.0;                                         // максимум total\n",
        "\n",
        "    MPI_Reduce(&compute_time, &comp_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n",
        "    MPI_Reduce(&comm_time,    &comm_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n",
        "    MPI_Reduce(&total_time,   &total_max,1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    if (rank == 0)                                                  // вывод на root\n",
        "    {\n",
        "        std::string op = (use_allreduce == 0) ? \"Reduce\" : \"Allreduce\"; // что использовали\n",
        "\n",
        "        std::cout << \"mode=\" << mode\n",
        "                  << \" procs=\" << size\n",
        "                  << \" N_total=\" << N_total\n",
        "                  << \" local_n~=\" << base\n",
        "                  << \" op=\" << op << \"\\n\";\n",
        "\n",
        "        std::cout << \"time_total_s=\" << total_max\n",
        "                  << \" time_compute_s=\" << comp_max\n",
        "                  << \" time_comm_s=\" << comm_max << \"\\n\";\n",
        "\n",
        "        // покажем результат агрегации (чисто для уверенности)\n",
        "        std::cout << \"sum=\" << gsum << \" min=\" << gmin << \" max=\" << gmax << \"\\n\";\n",
        "\n",
        "        // доля коммуникаций\n",
        "        double comm_share = (total_max > 0.0) ? (comm_max / total_max) : 0.0; // доля comm\n",
        "        std::cout << \"comm_share=\" << comm_share << \"\\n\\n\";\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();                                                  // конец MPI\n",
        "    return 0;                                                        // выход\n",
        "}"
      ],
      "metadata": {
        "id": "OY9AAlwvRuRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e5ad28-6499-4d47-dfbc-62091ef18d86"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task4.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ -O2 task4.cpp -o task4"
      ],
      "metadata": {
        "id": "NAu7SaRqcUDt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 1 ./task4 strong 50000000 0\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./task4 strong 50000000 0\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./task4 strong 50000000 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4FzV7G9caOO",
        "outputId": "3cc0f253-b6ff-47ca-a658-006a31e479cc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mode=strong procs=1 N_total=50000000 local_n~=50000000 op=Reduce\n",
            "time_total_s=0.0700232 time_compute_s=0.0700131 time_comm_s=9.331e-06\n",
            "sum=2.49992e+07 min=1.79399e-08 max=1\n",
            "comm_share=0.000133256\n",
            "\n",
            "mode=strong procs=2 N_total=50000000 local_n~=25000000 op=Reduce\n",
            "time_total_s=0.0505728 time_compute_s=0.0504933 time_comm_s=7.8862e-05\n",
            "sum=2.49991e+07 min=6.95543e-09 max=1\n",
            "comm_share=0.00155937\n",
            "\n",
            "mode=strong procs=4 N_total=50000000 local_n~=12500000 op=Reduce\n",
            "time_total_s=0.0517997 time_compute_s=0.0457352 time_comm_s=0.00606321\n",
            "sum=2.50016e+07 min=5.48134e-11 max=1\n",
            "comm_share=0.117051\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 1 ./task4 strong 50000000 1\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./task4 strong 50000000 1\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./task4 strong 50000000 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mgu3ISacv8O",
        "outputId": "f3113655-7417-4070-df25-1e741c1fea51"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mode=strong procs=1 N_total=50000000 local_n~=50000000 op=Allreduce\n",
            "time_total_s=0.0709871 time_compute_s=0.0709765 time_comm_s=9.62e-06\n",
            "sum=2.49992e+07 min=1.79399e-08 max=1\n",
            "comm_share=0.000135518\n",
            "\n",
            "mode=strong procs=2 N_total=50000000 local_n~=25000000 op=Allreduce\n",
            "time_total_s=0.0503517 time_compute_s=0.0503095 time_comm_s=0.000177819\n",
            "sum=2.49991e+07 min=6.95543e-09 max=1\n",
            "comm_share=0.00353154\n",
            "\n",
            "mode=strong procs=4 N_total=50000000 local_n~=12500000 op=Allreduce\n",
            "time_total_s=0.0537572 time_compute_s=0.0513671 time_comm_s=0.0196138\n",
            "sum=2.50016e+07 min=5.48134e-11 max=1\n",
            "comm_share=0.364859\n",
            "\n"
          ]
        }
      ]
    }
  ]
}